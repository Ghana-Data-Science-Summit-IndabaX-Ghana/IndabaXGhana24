{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Intermediate: Machine Learning**\n",
        "\n",
        "### Authors: [Precious Darkwa](http://linkedin.com/in/darkwa-precious)\n",
        "\n",
        "\n",
        "### Content\n",
        "\n",
        "1. [Supervised Learning](#part1)\n",
        "  - [Linear Models](#part1.1)\n",
        "  - [Problem Statement](#part1.2)\n",
        "    * [Feature Descriptions](#part1.2.1)\n",
        "    * [Save model](#part1.2.2)\n",
        "    * [OOP Approach](#part1.2.3)\n",
        "    * [Procedural Approach](#part1.2.4)\n",
        "    * [Exercise](#part1.2.5)"
      ],
      "metadata": {
        "id": "nuC5KssfNG4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJda6sOV5IT"
      },
      "source": [
        "<a name=\"part1\"></a>\n",
        "\n",
        "\n",
        "# **Supervised Learning**\n",
        "Supervised learning is a branch of machine learning where the algorithm learns from labeled data, which means each input data point is associated with a corresponding target label. The goal of supervised learning is to learn a mapping from input variables to output variables.\n",
        "\n",
        "In supervised learning, there are two main classes of models:\n",
        "\n",
        "1. **Regression Models**: These models are used when the target variable is continuous. The goal of regression is to predict a continuous outcome. Examples of regression models include linear regression, polynomial regression, support vector regression (SVR), and ridge regression.\n",
        "\n",
        "2. **Classification Models**: These models are used when the target variable is categorical, meaning it belongs to a specific class or category. The goal of classification is to predict the class label of new data points. Examples of classification models include logistic regression, decision trees, random forests, support vector machines (SVM), k-nearest neighbors (KNN), and naive Bayes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub1-xo2peczv"
      },
      "source": [
        "<a name=\"part1.1\"></a>\n",
        "\n",
        "\n",
        "## **Linear Models**\n",
        "\n",
        "1. **Linear Regression**:\n",
        "   - Linear regression is a basic and widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables.\n",
        "   - It assumes a linear relationship between the independent variables and the dependent variable.\n",
        "   - The model equation is of the form: y = B0 + B1x1 + B2x2 + .... + Bnxn + E1 where Bi's are the coefficients, xi's are the independent variables and E is the error term\n",
        "\n",
        "   ```python\n",
        "   from sklearn.linear_model import LinearRegression\n",
        "\n",
        "   # Create a linear regression model\n",
        "   model = LinearRegression()\n",
        "\n",
        "   # Fit the model to the data\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Make predictions\n",
        "   predictions = model.predict(X_test)\n",
        "   ```\n",
        "\n",
        "2. **Logistic Regression**:\n",
        "   - Logistic regression is a linear model used for binary classification tasks.\n",
        "   - It models the probability that a given instance belongs to a particular class.\n",
        "   - Despite its name, it's a classification algorithm, not a regression algorithm.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "   # Create a logistic regression model\n",
        "   model = LogisticRegression()\n",
        "\n",
        "   # Fit the model to the data\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Make predictions\n",
        "   predictions = model.predict(X_test)\n",
        "   ```\n",
        "\n",
        "3. **Support Vector Machine (SVM)**:\n",
        "   - SVM is a powerful supervised learning algorithm used for both classification and regression tasks.\n",
        "   - In classification, it finds the hyperplane that best separates the classes.\n",
        "   - It can handle linear and non-linear data through the use of different kernel functions.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.svm import SVC\n",
        "\n",
        "   # Create an SVM classifier\n",
        "   model = SVC(kernel='linear')\n",
        "\n",
        "   # Fit the model to the data\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   # Make predictions\n",
        "   predictions = model.predict(X_test)\n",
        "   ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glJvkd-JXfzr"
      },
      "source": [
        "<a name=\"part1.2\"></a>\n",
        "\n",
        "\n",
        "## **Problem Statement**:\n",
        "The challenge at hand involves addressing the surge in building collapses within Lagos and major cities in Nigeria. In response, there's a need to develop a predictive model that can forecast whether a building will file an insurance claim within a specific timeframe. The objective is to ascertain the likelihood of a building experiencing at least one insurance claim during its insured period based on various building characteristics. The target variable, \"Claim,\" will be binary:\n",
        "\n",
        "1 if the building has at least one insurance claim during the insured period.\n",
        "\n",
        "0 if the building does not file any insurance claims during the insured period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w37vOfHNV5Yx"
      },
      "source": [
        "<a name=\"part1.2.1\"></a>\n",
        "\n",
        "\n",
        "### **Descriptions for each features in the dataset**\n",
        "\n",
        "1. **Building_Painted**: A binary variable indicating whether the building has been painted or not. In the data provided, \"N\" represents \"Not painted\" and \"V\" represents \"Painted\".\n",
        "\n",
        "2. **Building_Fenced**: A binary variable indicating whether the building is fenced or not. In the data provided, \"N\" represents \"Not fenced\" and \"V\" represents \"Fenced\".\n",
        "\n",
        "3. **Garden**: A binary variable indicating the presence or absence of a garden in or around the building. In the data provided, \"O\" represents \"No garden\" and \"V\" represents \"Has a garden\".\n",
        "\n",
        "4. **Settlement**: A categorical variable indicating the type of settlement where the building is located. In the data provided, \"U\" represents \"Urban\" and \"R\" represents \"Rural\".\n",
        "\n",
        "5. **NumberOfWindows**: A categorical variable indicating the number of windows in the building. In the data provided, numerical values represent the count of windows.\n",
        "\n",
        "6. **Customer Id**: An identifier assigned to each customer. It uniquely identifies each entry in the dataset.\n",
        "\n",
        "7. **YearOfObservation**: The year in which the observation or data collection was made. It provides temporal context for the recorded data.\n",
        "\n",
        "8. **Insured_Period**: The duration for which the building is insured, typically represented in years. It indicates the length of time the insurance policy covers.\n",
        "\n",
        "9. **Residential**: A binary variable indicating whether the building is used for residential purposes or not. In the data provided, \"0\" represents \"Not residential\" and \"1\" represents \"Residential\".\n",
        "\n",
        "10. **Building Dimension**: The physical size or dimensions of the building, typically measured in square meters or a similar unit. It provides quantitative information about the building's size.\n",
        "\n",
        "11. **Building_Type**: A categorical variable indicating the type or classification of the building. In the data provided, numerical values represent different building types.\n",
        "\n",
        "12. **Date_of_Occupancy**: The year in which the building was occupied or became operational. It provides information on the age of the building.\n",
        "\n",
        "13. **Geo_Code**: A code representing the geographical location or region where the building is situated. It helps identify the geographic distribution of buildings.\n",
        "\n",
        "14. **Claim**: The target variable indicating whether the building has filed an insurance claim (1) or not (0) during the insured period. It serves as the outcome variable for predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u2yfX50V8sq",
        "outputId": "497801ec-7667-46ac-fc5f-54b78b49fee4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTvJou9Ut7G0"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/My Drive/Insurance Prediction/train_data.csv'\n",
        "file_path_test = '/content/drive/My Drive/Insurance Prediction/test_data.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH9h7Yoj1Eph"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nVxucKPumGp"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3ct2Ia1urVy"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c048fqJSvC8Z"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NBSLkZl0d7l"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['NumberOfWindows'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtVmtJqM3B4b"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['Geo_Code'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jl2MzDI3H9D"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['Building_Painted'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EanqiCaz3IAA"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['Building_Fenced'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPKxdXTR3IDS"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['Settlement'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpV3USoe3B73"
      },
      "outputs": [],
      "source": [
        "pd.unique(df['Garden'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJGqqLJ908bu"
      },
      "outputs": [],
      "source": [
        "# Strip spaces and handle special cases in the 'NumberOfWindows' column\n",
        "df['NumberOfWindows'] = df['NumberOfWindows'].str.strip().replace({\n",
        "    '.': np.nan,\n",
        "    '>=10': 10\n",
        "}).astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmrQZ6Uq5gbj"
      },
      "outputs": [],
      "source": [
        "numerical_columns = df.select_dtypes(include=['float64','int64']).columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV3mxeNs6DZS"
      },
      "outputs": [],
      "source": [
        "numerical_impute = SimpleImputer(strategy='median')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM4h_f9U6Z-Z"
      },
      "outputs": [],
      "source": [
        "for col in numerical_columns:\n",
        "  df[col] = numerical_impute.fit_transform(df[[col]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUBWP7qf6o75"
      },
      "outputs": [],
      "source": [
        "for col in categorical_columns:\n",
        "  df[col] = df[col].fillna(df[col].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cx6DQak9ya6"
      },
      "outputs": [],
      "source": [
        "label = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAf0GVyl99Fz"
      },
      "outputs": [],
      "source": [
        "for col in categorical_columns:\n",
        "  df[col] = label.fit_transform(df[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2Zp4em999In"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUVwWwpg09Qx"
      },
      "outputs": [],
      "source": [
        "# Define features and target variable\n",
        "X = df.drop(columns=['Customer Id', 'Claim'])\n",
        "y = df['Claim']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "choSc0er1KXx"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC57tXOc-PPj"
      },
      "outputs": [],
      "source": [
        "# Build and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDQ0Uliy-PSg"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLZ5LcxS1KaE"
      },
      "outputs": [],
      "source": [
        "# y_test, y_pred, and y_pred_prob are defined\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"ROC AUC Score:\", round(roc_auc,2))\n",
        "print(\"accuracy : \" + str(round(accuracy,2) * 100) + \" %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtIvMI0A09WC"
      },
      "outputs": [],
      "source": [
        "# Print the classification report\n",
        "classification_report_output = classification_report(y_test, y_pred)\n",
        "print(classification_report_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpzaWHIw09YR"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6T95OXMaydx"
      },
      "source": [
        "<a name=\"part1.2.2\"></a>\n",
        "\n",
        "### **Saving the Model using Pickle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgpuIpDo2U6N"
      },
      "outputs": [],
      "source": [
        "# Save the model to a file\n",
        "pickle_file = 'logistic_regression_model.pkl'\n",
        "with open(pickle_file, 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Load the model from the file\n",
        "with open(pickle_file, 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "y_pred_prob = loaded_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Print the classification report\n",
        "classification_report_output = classification_report(y_test, y_pred)\n",
        "accuracy = round(accuracy_score(y_test, y_pred),2)\n",
        "print(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpUUHQwan5F"
      },
      "source": [
        "<a name=\"part1.2.2\"></a>\n",
        "\n",
        "### **Another Method to Save the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S2TQFP_2BZ6"
      },
      "outputs": [],
      "source": [
        "# Save the model to a file\n",
        "joblib_file = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model,joblib_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaoZay2D19xa"
      },
      "outputs": [],
      "source": [
        "# Load the model from the file\n",
        "loaded_model = joblib.load(joblib_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAAgdbqIIZh-"
      },
      "source": [
        "<a name=\"part1.2.3\"></a>\n",
        "\n",
        "\n",
        "### **OOP APPROACH FOR THE MODEL CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkCXxrkkzgwV"
      },
      "outputs": [],
      "source": [
        "class MachineLearningPipeline:\n",
        "    def __init__(self, model_path):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.label_encoders = None\n",
        "\n",
        "    def load_data(self, data_path):\n",
        "        # Load the dataset\n",
        "        return pd.read_csv(data_path)\n",
        "\n",
        "    def preprocess_data(self, data, training=True):\n",
        "        # Identify numerical and categorical columns\n",
        "        numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "        categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Impute missing values for numerical columns\n",
        "        numerical_imputer = SimpleImputer(strategy='median')\n",
        "        for col in numerical_columns:\n",
        "            data[col] = numerical_imputer.fit_transform(data[[col]])\n",
        "\n",
        "        # Impute missing values for categorical columns\n",
        "        for col in categorical_columns:\n",
        "            data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "        # Convert categorical variables to numerical using LabelEncoder\n",
        "        self.label_encoders = {}\n",
        "        for col in categorical_columns:\n",
        "            le = LabelEncoder()\n",
        "            data[col] = le.fit_transform(data[col])\n",
        "            self.label_encoders[col] = le\n",
        "\n",
        "        if training:\n",
        "            # Split the data into features and target\n",
        "            X = data.drop(['Customer Id', 'Claim'], axis=1)\n",
        "            y = data['Claim']\n",
        "            return X, y\n",
        "        else:\n",
        "            # For new data prediction\n",
        "            X_new = data.drop(['Customer Id'], axis=1)\n",
        "            return X_new\n",
        "\n",
        "    def train_model(self, X, y):\n",
        "        # Train a logistic regression model\n",
        "        self.model = LogisticRegression(max_iter=1000)\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def save_model(self):\n",
        "        # Save the model to a file\n",
        "        with open(self.model_path, 'wb') as file:\n",
        "            pickle.dump(self.model, file)\n",
        "\n",
        "    def evaluate_model(self, X, y):\n",
        "        # Predict and evaluate the model\n",
        "        y_pred = self.model.predict(X)\n",
        "        y_pred_prob = self.model.predict_proba(X)[:, 1]\n",
        "\n",
        "        conf_matrix = confusion_matrix(y, y_pred)\n",
        "        roc_auc = round(roc_auc_score(y, y_pred_prob),2)\n",
        "        classification_report_output = classification_report(y, y_pred)\n",
        "        accuracy = round(accuracy_score(y, y_pred),2) * 100\n",
        "\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(conf_matrix)\n",
        "        print(\"\\nROC AUC Score:\")\n",
        "        print(roc_auc)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report_output)\n",
        "        print(f\"\\nAccuracy: {accuracy} %\")\n",
        "\n",
        "    def load_model(self):\n",
        "        # Load the model from a file\n",
        "        with open(self.model_path, 'rb') as file:\n",
        "            self.model = pickle.load(file)\n",
        "\n",
        "# Usage\n",
        "pipeline = MachineLearningPipeline('logistic_regression_model.pkl')\n",
        "data = pipeline.load_data(file_path)\n",
        "X, y = pipeline.preprocess_data(data, training=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Train model\n",
        "pipeline.train_model(X_train, y_train)\n",
        "pipeline.save_model()\n",
        "\n",
        "# Evaluate model\n",
        "pipeline.evaluate_model(X_test, y_test)\n",
        "\n",
        "# Load model\n",
        "pipeline.load_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441dzYFhIrL6"
      },
      "source": [
        "<a name=\"part1.2.4\"></a>\n",
        "\n",
        "### **PROCEDURAL APPROACH FOR MODEL CREATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ssz0Cq1_i8k"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path):\n",
        "    # Load the dataset\n",
        "    return pd.read_csv(data_path)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Your preprocessing code\n",
        "    # Identify numerical and categorical columns\n",
        "    numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Impute missing values for numerical columns\n",
        "    numerical_imputer = SimpleImputer(strategy='median')\n",
        "    for col in numerical_columns:\n",
        "        data[col] = numerical_imputer.fit_transform(data[[col]])\n",
        "\n",
        "    # Impute missing values for categorical columns\n",
        "    for col in categorical_columns:\n",
        "        data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "    # Convert categorical variables to numerical using LabelEncoder\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    return data, label_encoders\n",
        "\n",
        "def train_model(X, y):\n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "def save_model(model, model_path):\n",
        "    # Save the model to a file\n",
        "    with open(model_path, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    # Predict and evaluate the model\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred_prob = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    conf_matrix = confusion_matrix(y, y_pred)\n",
        "    roc_auc = round(roc_auc_score(y, y_pred_prob), 2)\n",
        "    classification_report_output = classification_report(y, y_pred)\n",
        "    accuracy = round(accuracy_score(y, y_pred),2) * 100\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nROC AUC Score:\")\n",
        "    print(roc_auc)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report_output)\n",
        "    print(f\"\\nAccuracy: {accuracy} %\")\n",
        "\n",
        "def load_model(model_path):\n",
        "    # Load the model from a file\n",
        "    with open(model_path, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "# Usage\n",
        "data_path = file_path\n",
        "model_path = 'logistic_regression_model.pkl'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data(data_path)\n",
        "data, _ = preprocess_data(data)\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop(['Customer Id', 'Claim'], axis=1)\n",
        "y = data['Claim']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Train model\n",
        "model = train_model(X_train, y_train)\n",
        "# Save model\n",
        "save_model(model, model_path)\n",
        "\n",
        "# Evaluate model\n",
        "evaluate_model(model, X_test, y_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWGfn_35ZSrl"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "# Evaluate loaded model\n",
        "evaluate_model(loaded_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piDWRjydUwQi"
      },
      "source": [
        "### **PROCEDURAL APPROACH FOR THE TEST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XElJ37sY78fD"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    # Load the model from a file\n",
        "    with open(model_path, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # Identify numerical and categorical columns\n",
        "    numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Impute missing values for numerical columns\n",
        "    numerical_imputer = SimpleImputer(strategy='median')\n",
        "    for col in numerical_columns:\n",
        "        data[col] = numerical_imputer.fit_transform(data[[col]])\n",
        "\n",
        "    # Impute missing values for categorical columns\n",
        "    for col in categorical_columns:\n",
        "        data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "    # Convert categorical variables to numerical using LabelEncoder\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    return data, label_encoders\n",
        "\n",
        "def save_predictions(predictions, output_path):\n",
        "    # Save predictions to an excel file\n",
        "    predictions.to_excel(output_path, index=False)\n",
        "\n",
        "def predict(data_path, model_path, output_path):\n",
        "    # Load data\n",
        "    data = pd.read_csv(data_path)\n",
        "\n",
        "    # Preprocess data\n",
        "    data, _ = preprocess_data(data)\n",
        "\n",
        "    # Load model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Make predictions\n",
        "    X = data.drop('Customer Id', axis=1)  # Assuming 'Customer Id' is present in data\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Create DataFrame with customer_id and predictions\n",
        "    customer_id = data['Customer Id']\n",
        "    predictions_df = pd.DataFrame({'Customer Id': customer_id, 'Predictions': predictions})\n",
        "\n",
        "    # Save predictions to excel\n",
        "    save_predictions(predictions_df, output_path)\n",
        "\n",
        "# Usage\n",
        "data_path = file_path_test\n",
        "model_path = 'logistic_regression_model.pkl'\n",
        "output_path = 'predictions.xlsx'\n",
        "predict(data_path, model_path, output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lg1A6J8U72-"
      },
      "source": [
        "<a name=\"part1.2.5\"></a>\n",
        "\n",
        "### **EXERCISE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeddsnAwU8tr"
      },
      "source": [
        "### **OOP APPROACH FOR THE TEST DATA**\n",
        "\n",
        "Write the code for the object-oriented programming (OOP) approach for the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congrats! That's it for this tutorial.\n",
        "\n",
        "\n",
        "### Author(s):\n",
        "Precious Darkwa, Data Science/Analytics Instructor @ Blossom Academy\n",
        "\n",
        "Email: preciousdarkwa@gmail.com\n",
        "\n",
        "---\n",
        "\n",
        "*This notebook was originally created by Ghana Data Science Summit for the [IndabaX Ghana](https://www.indabaxghana.com/) 2024 Conference and is published under [MIT license](https://choosealicense.com/licenses/mit/).*"
      ],
      "metadata": {
        "id": "lrxl9wMvPuUZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}