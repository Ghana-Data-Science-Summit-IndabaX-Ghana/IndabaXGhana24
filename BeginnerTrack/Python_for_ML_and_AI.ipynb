{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAIubEq1NzR4"
      },
      "source": [
        "# **Credit Card Fraud Detection with Python**\n",
        "\n",
        "## Content\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [What are Libraries and Why Do We Need Them?](#libraries)\n",
        "3. [Installing and Importing Libraries](#installing-importing)\n",
        "4. [About the Libraries](#about-libraries)\n",
        "   - 4.1 [Numpy](#numpy)\n",
        "   - 4.2 [Pandas](#pandas)\n",
        "   - 4.3 [Matplotlib](#matplotlib)\n",
        "   - 4.4 [Seaborn](#seaborn)\n",
        "   - 4.5 [Scikit-learn](#scikit-learn)\n",
        "5. [Steps involved in building an ML Model](#ml)\n",
        "6. [Loading the Dataset](#loading-dataset)\n",
        "6. [Exploring the Dataset](#exploring-dataset)\n",
        "7. [Data Visualization](#data-visualization)\n",
        "8. [Data Preprocessing](#data-preprocessing)\n",
        "   - 8.1 [Handling Missing Values](#handling-missing-values)\n",
        "   - 8.2 [Splitting the Data](#splitting-data)\n",
        "   - 8.3 [Dealing with Imbalanced Data](#dealing-imbalanced-data)\n",
        "   - 8.4 [Standardizing the Features](#standardizing-features)\n",
        "9. [Building and Training the Model](#building-training-model)\n",
        "10. [Making Predictions](#making-predictions)\n",
        "11. [Evaluating the Model](#evaluating-model)\n",
        "   - 11.1 [Classification Report](#classification-report)\n",
        "   - 11.2 [Confusion Matrix](#confusion-matrix)\n",
        "     - 11.2.1 [Layout of a Confusion Matrix](#aconfusion-matrix)\n",
        "   - 11.3 [ROC Curve and AUC](#roc-auc)\n",
        "12. [User Input for Prediction](#user-input)\n",
        "13. [Conclusion](#conclusion)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction <a name=\"introduction\"></a>\n",
        "In this tutorial, we will walk through a complete machine learning project to detect credit card fraud. We will use Python and several popular libraries, including scikit-learn, pandas, numpy, matplotlib, and seaborn. By the end of this tutorial, you will have a good understanding of how to use these libraries to build and evaluate a machine learning model.\n",
        "\n",
        "---\n",
        "\n",
        "## What are Libraries and Why Do We Need Them? <a name=\"libraries\"></a>\n",
        "\n",
        "\n",
        "Libraries are collections of pre-written code that you can use to perform common tasks. They save you time and effort by providing ready-made functions and tools. In Python, libraries are crucial for data science and machine learning because they offer a wide range of functionalities, from data manipulation to advanced mathematical operations and visualization.\n",
        "\n",
        "**Dependencies?**\n",
        "\n",
        "These libraries provide specific functionalities in programming, while dependencies are external components essential for a project's proper functioning, ensuring compatibility and reliability. Examples include NumPy and Pandas as libraries, and specific versions of these as dependencies in Python projects.\n",
        "\n",
        "---\n",
        "\n",
        "## Installing and Importing Libraries <a name=\"installing-importing\"></a>\n",
        "First, let's install and import the necessary libraries.\n",
        "\n",
        "### Installing Libraries\n",
        "To install libraries, you can use the `pip` command. In Google Colab, you can install libraries directly in a code cell.\n",
        "\n",
        "```python\n",
        "!pip install numpy pandas matplotlib seaborn scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIMsQBV4PJZ9"
      },
      "source": [
        "**Importing Libraries**\n",
        "\n",
        "Now, let's import the libraries we will use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-nJL5ZNVk-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "301-js-CPn3h"
      },
      "source": [
        "#About the Libraries <a name=\"about-libraries\"></a>\n",
        "\n",
        "**Numpy <a name=\"numpy\"></a>**\n",
        "\n",
        "Numpy is a powerful library for numerical computations. It provides support for arrays, matrices, and many mathematical functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWaju-P5S-rA"
      },
      "source": [
        "![Numpy Logo](https://th.bing.com/th/id/OIP.Q5FR3UBk0lctzx8vwbcMTQHaFO?rs=1&pid=ImgDetMain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ1nipc8TpKO"
      },
      "source": [
        "**Pandas <a name=\"pandas\"></a>**\n",
        "\n",
        "Pandas is a library used for data manipulation and analysis. It offers data structures like DataFrames, which are great for handling tabular data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRPC_bMFTwBS"
      },
      "source": [
        "![Pandas Logo](https://th.bing.com/th/id/OIP.UhPrO71Q-C3PgpwzRgSnBAHaD4?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJfQZ0-CUz6t"
      },
      "source": [
        "**Matplotlib <a name=\"matplotlib\"></a>**\n",
        "\n",
        "Matplotlib is a plotting library used for creating static, interactive, and animated visualizations in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7TIhc07UzuF"
      },
      "source": [
        "**Seaborn <a name=\"seaborn\"></a>**\n",
        "\n",
        "Seaborn is a statistical data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr2OYtoKUzgO"
      },
      "source": [
        "**Scikit-learn <a name=\"scikit-learn\"></a>**\n",
        "\n",
        "Scikit-learn is a machine learning library that provides simple and efficient tools for data mining and data analysis.\n",
        "\n",
        "It is a comprehensive machine learning library that includes various algorithms for classification, regression, clustering, and more. It also provides tools for model selection, evaluation, and preprocessing of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RZ5ppbMYe6Z"
      },
      "source": [
        "#The steps involved in building Machine Learning models <a name=\"ml\"></a>\n",
        "![ML immage](https://th.bing.com/th/id/OIP.g8FbnR-xikAiWqcsyRCq4QHaDa?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsoHzbX-Uygx"
      },
      "source": [
        "#**Loading the Dataset <a name=\"loading-dataset\"></a>**\n",
        "\n",
        "We will use the Credit Card Fraud Detection dataset, which contains transactions made by credit cards in September 2013 by European cardholders. The dataset is highly unbalanced, with the positive class (frauds) accounting for 0.172% of all transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2pG4nO8NpJQ"
      },
      "outputs": [],
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv'\n",
        "df = pd.read_csv(url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5o4TaUOZofT"
      },
      "source": [
        "**Exploring the Dataset <a name=\"exploring-dataset\"></a>**\n",
        "\n",
        "To understand the dataset, we'll use Pandas to load the data and perform initial exploratory data analysis (EDA). Functions like head(), info(), and describe() will help us get insights into the dataset's structure, features, and basic statistics.\n",
        "\n",
        "Let's take a look at the first few rows of the dataset to understand its structure.\n",
        "\n",
        "And the last few rows respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At8NpplSNpXJ",
        "outputId": "ec2192b2-9b14-43ce-aa42-2664ad51c240"
      },
      "outputs": [],
      "source": [
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z816SKjWbCbw"
      },
      "source": [
        "We can also check the dataset's information and summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys4r_b3WbFE7"
      },
      "outputs": [],
      "source": [
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvA0Tg7RbG15"
      },
      "outputs": [],
      "source": [
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEI1fCjMbJUu"
      },
      "source": [
        "**Data Visualization <a name=\"data-visualization\"></a>**\n",
        "\n",
        "Visualizing data helps in understanding distributions, correlations, and patterns. Matplotlib and Seaborn provide functions to create various plots such as histograms, scatter plots, and heatmaps, enabling us to visualize relationships and anomalies in the data.\n",
        "\n",
        "**Let's visualize the class distribution to see the imbalance in the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ldP2nDlpbOqg",
        "outputId": "76f30012-13ad-4ecb-adeb-7d3dcf4564c5"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Class Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v_c7I8vbg3G"
      },
      "source": [
        "#**Data Preprocessing <a name=\"data-preprocessing\"></a>**\n",
        "\n",
        "**Handling Missing Values <a name=\"handling-missing-values\"></a>**\n",
        "\n",
        "Check for missing values and handle them if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kavkgL81bfh9",
        "outputId": "c51de1a9-3d88-4d72-e479-5554192fa249"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsLz-mZjd8Xk"
      },
      "source": [
        "**Splitting the Data <a name=\"splitting-data\"></a>**\n",
        "\n",
        "We will split the dataset into features (X) and target variable (y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hiaR6UBeC3X"
      },
      "outputs": [],
      "source": [
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49nWumkPb8Z0"
      },
      "source": [
        "#**Dealing with Imbalanced Data <a name=\"dealing-imbalanced-data\"></a>**\n",
        "\n",
        "---\n",
        "**What is Imbalanced Data**: Imbalanced data occurs when one class (e.g., fraudulent transactions) is underrepresented compared to another class (e.g., non-fraudulent transactions).\n",
        "\n",
        "---\n",
        "**Why Handle Imbalanced Data**: Imbalanced datasets can lead to biased models that favor the majority class. Techniques like oversampling (SMOTE) and undersampling help balance the dataset, ensuring better model performance.\n",
        "\n",
        "---\n",
        "\n",
        "**SMOTE (Synthetic Minority Over-sampling Technique)**: SMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples. This technique helps in improving the model's ability to detect fraud cases.\n",
        "\n",
        "To handle the imbalanced dataset, we will use this technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoNIUoRUNLnT"
      },
      "source": [
        "smote function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klJwsmVbdDhO"
      },
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhgCFybSePqI"
      },
      "source": [
        "Then, we will split the data into training and testing sets.\n",
        "\n",
        "Before training the model, we split the dataset into training and testing sets using `train_test_split()` from Scikit-learn. This ensures that the model's performance can be evaluated on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fb07jDreMbV"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6iSKh58eUeN"
      },
      "source": [
        "**Standardizing the Features <a name=\"standardizing-features\"></a>**\n",
        "\n",
        "Standardization is important for many machine learning algorithms.\n",
        "\n",
        "Standardization (or normalization) of features ensures that all features contribute equally to the model training process. We use StandardScaler from Scikit-learn to standardize numerical features, making them have zero mean and unit variance.\n",
        "\n",
        "\n",
        " We will use StandardScaler from scikit-learn to standardize the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkahKt2zea4i"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIpXRnZCeeMo"
      },
      "source": [
        "#**Building and Training the Model <a name=\"building-training-model\"></a>**\n",
        "\n",
        "Machine learning models learn patterns from data to make predictions. We'll use algorithms like RandomForestClassifier, which builds multiple decision trees and merges them together to improve predictive accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**Using Advanced Algorithms <a name=\"advanced-algorithms\"></a>**\n",
        "\n",
        "We will use a Random Forest Classifier and a Gradient Boosting Classifier for this task. These are ensemble methods that combine multiple models to improve accuracy and prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DhWRpYUleyao",
        "outputId": "a4f2dfcc-d6bc-4f2a-f899-bd540d804673"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "EOgzFsImifcK",
        "outputId": "35ba3a0e-0af6-4b9d-a2e6-4f9b9b25d111"
      },
      "outputs": [],
      "source": [
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train_scaled, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-dTZT8jioEu"
      },
      "source": [
        "**Making Predictions <a name=\"making-predictions\"></a>**\n",
        "\n",
        "Once the model is trained, it can predict whether a transaction is fraudulent or not based on its input features. Predictions are made using the predict() method of the trained model.\n",
        "\n",
        "\n",
        "Use the trained models to make predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmTtrlRFizA0"
      },
      "outputs": [],
      "source": [
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "y_pred_gb = gb_model.predict(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czn2Jfwi1oD"
      },
      "source": [
        "#**Evaluating the Models <a name=\"evaluating-model\"></a>**\n",
        "\n",
        "**Classification Report <a name=\"classification-report\"></a>**\n",
        "\n",
        "A classification report provides metrics such as precision, recall, F1-score, and support for each class. It helps in understanding the model's performance in detecting fraudulent transactions.\n",
        "\n",
        "---\n",
        "**Precision**:\n",
        "\n",
        "What it is: Precision measures the accuracy of the positive predictions. It tells us what proportion of the predicted positive cases were actually positive.\n",
        "Why it matters: High precision means that when the model predicts a positive case, it is usually correct. It’s important when the cost of false positives is high.\n",
        "Example: If a fraud detection system identifies 10 transactions as fraudulent and 8 of them are actually fraudulent, the precision is 80%.\n",
        "\n",
        "---\n",
        "\n",
        "**Recall**:\n",
        "\n",
        "What it is: Recall (also known as sensitivity or true positive rate) measures the ability of the model to identify all actual positive cases. It tells us what proportion of the actual positive cases were correctly identified by the model.\n",
        "Why it matters: High recall means that the model catches most of the actual positive cases. It’s important when the cost of false negatives is high.\n",
        "Example: If there are 20 actual fraudulent transactions and the model identifies 15 of them, the recall is 75%.\n",
        "\n",
        "---\n",
        "\n",
        "**F1-Score**:\n",
        "\n",
        "What it is: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns.\n",
        "Why it matters: It is useful when you need a balance between precision and recall and there is an uneven class distribution.\n",
        "Example: If a model has a precision of 70% and a recall of 80%, the F1-score will be around 74%.\n",
        "\n",
        "---\n",
        "\n",
        "**Support**:\n",
        "\n",
        "What it is: Support is the number of actual occurrences of each class in the dataset. It gives you an idea of how many instances of each class are present.\n",
        "Why it matters: Knowing the support helps in understanding the context of precision, recall, and F1-score, especially if the classes are imbalanced.\n",
        "Example: If there are 1000 transactions and 50 of them are fraudulent, the support for the fraud class is 50 and for the non-fraud class is 950.\n",
        "\n",
        "---\n",
        "\n",
        "Generate a classification report to evaluate the model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyE3wNPSjDqX",
        "outputId": "aa406589-e276-49f7-d6a3-8fe9d40b3ae5"
      },
      "outputs": [],
      "source": [
        "print(\"Random Forest Classifier:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWKOqBoUjGi9",
        "outputId": "aac044ed-8087-475e-b76a-282235ca20e3"
      },
      "outputs": [],
      "source": [
        "print(\"Gradient Boosting Classifier:\")\n",
        "print(classification_report(y_test, y_pred_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G91KtiYijK9A"
      },
      "source": [
        "**Confusion Matrix <a name=\"confusion-matrix\"></a>**\n",
        "\n",
        "A confusion matrix is a table that summarizes the actual and predicted classifications done by a classification model. It helps visualize the performance of the model by showing the counts of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Visualize the confusion matrix for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "K77r646hjb4a",
        "outputId": "de2b57b8-67a2-4b16-b7b3-14671ff8f81a"
      },
      "outputs": [],
      "source": [
        "# Predict using the models\n",
        "rf_pred = rf_model.predict(X_test_scaled)\n",
        "gb_pred = gb_model.predict(X_test_scaled)\n",
        "\n",
        "# Create confusion matrices\n",
        "rf_cm = confusion_matrix(y_test, rf_pred)\n",
        "gb_cm = confusion_matrix(y_test, gb_pred)\n",
        "\n",
        "# Display confusion matrices\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=['Non-Fraud', 'Fraud']).plot(cmap=plt.cm.Blues, ax=plt.gca())\n",
        "plt.title('Random Forest Classifier')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ConfusionMatrixDisplay(confusion_matrix=gb_cm, display_labels=['Non-Fraud', 'Fraud']).plot(cmap=plt.cm.Blues, ax=plt.gca())\n",
        "plt.title('Gradient Boosting Classifier')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ZR0geTZ4K5"
      },
      "source": [
        "\n",
        "\n",
        "**Layout of a Confusion matrix<a name=\"aconfusion-matrix\"></a>**\n",
        "\n",
        "The four squares in a confusion matrix represent the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These counts are crucial for understanding the performance of a classification model. Here’s a detailed breakdown of each quadrant in the context of fraud detection:\n",
        "\n",
        "#Explanation of Each Quadrant\n",
        "**True Negative (TN)**\n",
        "\n",
        "Location: Top-left\n",
        "Description: The number of correctly predicted non-fraudulent transactions. Both the actual and predicted values are negative (non-fraud).\n",
        "Significance: Indicates how well the model identifies legitimate transactions.\n",
        "\n",
        "---\n",
        "\n",
        "**False Positive (FP)**\n",
        "\n",
        "Location: Top-right\n",
        "Description: The number of non-fraudulent transactions incorrectly predicted as fraudulent. The actual value is negative (non-fraud), but the predicted value is positive (fraud).\n",
        "Significance: Indicates the instances where the model mistakenly flags legitimate transactions as fraud.\n",
        "\n",
        "---\n",
        "**False Negative (FN)**\n",
        "\n",
        "Location: Bottom-left\n",
        "Description: The number of fraudulent transactions incorrectly predicted as non-fraudulent. The actual value is positive (fraud), but the predicted value is negative (non-fraud).\n",
        "Significance: Indicates the instances where the model fails to detect fraud . These are missed fraud cases, which can be more problematic than FPs.\n",
        "\n",
        "---\n",
        "\n",
        "**True Positive (TP)**\n",
        "\n",
        "Location: Bottom-right\n",
        "Description: The number of correctly predicted fraudulent transactions. Both the actual and predicted values are positive (fraud).\n",
        "Significance: Indicates how well the model detects actual fraudulent transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDJLuF_aCIX"
      },
      "source": [
        "                 Predicted Negative  | Predicted Positive\n",
        "                 Actual Negative |        TN         |         FP\n",
        "\n",
        "                 Actual Positive |        FN         |         TP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFVgF-KRjf5o"
      },
      "source": [
        "**ROC Curve and AUC <a name=\"roc-auc\"></a>**\n",
        "\n",
        "The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold values. AUC (Area Under the ROC Curve) quantifies the model's ability to distinguish between classes, with higher values indicating better performance.\n",
        "\n",
        "Plot ROC curves and calculate AUC for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "BXt1xeRpjl4J",
        "outputId": "1db16a84-7971-4acb-8d72-7f2715961ed1"
      },
      "outputs": [],
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot ROC Curve for Random Forest\n",
        "RocCurveDisplay.from_estimator(rf_model, X_test_scaled, y_test, ax=axes[0])\n",
        "axes[0].set_title('Random Forest Classifier ROC Curve')\n",
        "\n",
        "# Plot ROC Curve for Gradient Boosting\n",
        "RocCurveDisplay.from_estimator(gb_model, X_test_scaled, y_test, ax=axes[1])\n",
        "axes[1].set_title('Gradient Boosting Classifier ROC Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNJAaUfajqHM"
      },
      "source": [
        "**User Input for Prediction <a name=\"user-input\"></a>**\n",
        "\n",
        "Allow users to input data for prediction using the trained model.\n",
        "\n",
        "We will define a function to take user input, preprocess it, and make a prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRY6xn04j7mw",
        "outputId": "c1c88eb3-05cb-45e4-9b53-4c06bac83e1d"
      },
      "outputs": [],
      "source": [
        "def get_user_input():\n",
        "    user_input = input(\"Enter the transaction data as a list of floats separated by commas: \")\n",
        "    user_data = np.array([float(i) for i in user_input.split(',')]).reshape(1, -1)\n",
        "    user_data_scaled = scaler.transform(user_data)\n",
        "    prediction = rf_model.predict(user_data_scaled)\n",
        "    return \"Fraud\" if prediction[0] == 1 else \"Not Fraud\"\n",
        "\n",
        "print(get_user_input())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkI8txqtrErj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress specific sklearn warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn.base')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecQhgB8WqMPx"
      },
      "outputs": [],
      "source": [
        "def get_user_input():\n",
        "    user_input = input(\"Enter the transaction data as a list of floats separated by commas: \")\n",
        "    user_data = np.array([float(i) for i in user_input.split(',')]).reshape(1, -1)\n",
        "    user_data_scaled = scaler.transform(user_data)\n",
        "    prediction = gb_model.predict(user_data_scaled)\n",
        "    return \"Fraud\" if prediction[0] == 1 else \"Not Fraud\"\n",
        "\n",
        "print(get_user_input())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZJpf6ek0xw"
      },
      "source": [
        "#Conclusion <a name=\"conclusion\"></a>\n",
        "\n",
        "In this tutorial, we explored the process of building a machine learning model for credit card fraud detection using Python and various libraries such as scikit-learn, pandas, numpy, matplotlib, and seaborn. We discussed data preprocessing, model training with advanced algorithms, evaluation metrics like classification report, confusion matrix, ROC curve, and AUC. Finally, we prepared for user input prediction, demonstrating a comprehensive approach to leveraging machine learning for fraud detection. We hope this tutorial has provided you with a good foundation for using Python libraries in machine learning projects. Thank you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG-7-bsBVKeS"
      },
      "source": [
        "---\n",
        "#Congrats! That's it for this tutorial.\n",
        "\n",
        "---\n",
        "<h1> Author(s):</h1>\n",
        "\n",
        "\n",
        "# **Abigail Akua Sika Boateng**\n",
        "\n",
        "\n",
        "**Social Media Handles**:\n",
        "* [Abigail (Akua Sika) Boateng](https://www.linkedin.com/in/abigail-boateng-287102244/)\n",
        "\n",
        "\n",
        "\n",
        "**Email:** abigailakuasikaboateng.com\n",
        "\n",
        "---\n",
        "# **Akua Serwaa Nkrumah**\n",
        "\n",
        "\n",
        "**Social Media Handles**:\n",
        "* [ Akua Serwaa Nkrumah ](https://www.linkedin.com/in/akua-serwaa-nkrumah-a4b782202)\n",
        "\n",
        "\n",
        "\n",
        "**Email:** nkrumahakua2002@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee3fRQRGQm-9"
      },
      "source": [
        "---\n",
        "\n",
        "*This notebook was originally created by Ghana Data Science Summit for the [IndabaX Ghana](https://www.indabaxghana.com/) 2024 Conference and is published under [MIT license](https://choosealicense.com/licenses/mit/).*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
