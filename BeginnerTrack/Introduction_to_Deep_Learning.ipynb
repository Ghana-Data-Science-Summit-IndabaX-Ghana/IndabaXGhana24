{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\n## Introduction to Building and Training a Transformer Model with PyTorch\n\nTransformers have significantly advanced natural language processing tasks, including text generation, translation, and summarization. This tutorial will guide you through the process of building and training a Transformer model from scratch using PyTorch. We will cover key components such as:\n\n- Input embeddings\n- Positional encoding\n- Multi-head attention\n- Feedforward blocks\n\nBy the end of this tutorial, you will gain a solid understanding of how these elements integrate to create a functional and efficient language model.","metadata":{"_uuid":"fbd3b0bc-b32c-46cd-9b3c-338002d14069","_cell_guid":"ab550f2c-e66a-4d95-b41e-b11d68fa1e26","id":"PrNd_TGM6KRc","trusted":true}},{"cell_type":"markdown","source":"**Deep Learning Training**","metadata":{"_uuid":"96d0652a-e508-44fe-9785-f3c5bf23ca69","_cell_guid":"c4be4636-90ee-4a9b-bad3-5f225a780746","id":"WgCnwbOAr9x2","trusted":true}},{"cell_type":"markdown","source":"# Table of Contents\n1. [Input Embeddings](#input-embeddings)\n2. [Positional Encoding](#positional-encoding)\n3. [Layer Normalization](#layer-normalization)\n4. [Feed Forward Block](#feed-forward-block)\n5. [Multi-Head Attention Block](#multi-head-attention-block)\n6. [Residual Connection](#residual-connection)\n7. [Projection Head](#projection-head)\n8. [Transformer Block](#transformer-block)\n9. [Building the Transformer](#building-the-transformer)\n10. [Sample Usage](#sample-usage)\n11. [Training the Transformer](#training-the-transformer)\n    1. [Data Preprocessing](#data-preprocessing)\n    2. [Model Training](#model-training)\n12. [Inference](#inference)","metadata":{"_uuid":"5fb682be-df77-4607-b411-27fad353f4fb","_cell_guid":"75b18585-4e97-4741-86be-bda632ca05ba","id":"g3Zq5nnTxxzf","trusted":true}},{"cell_type":"markdown","source":"**Install Dependencies**\n\nBefore we start building our Transformer model, we need to install and import the necessary libraries. Each of these libraries plays a specific role in our project:\n\n- **math**: Provides access to mathematical functions.\n- **pandas**: Used for data manipulation and analysis. It helps us handle and preprocess our datasets efficiently.\n- **torch**: The core library of PyTorch, used for building and training neural networks.\n- **torch.nn**: A sub-library of PyTorch that provides tools for building neural network layers.\n- **torch.nn.functional**: Contains functions used for building neural networks (often used for activation functions and other operations).\n- **torch.optim**: Contains optimization algorithms for training neural networks.\n- **torch.utils.data**: Provides tools like DataLoader and Dataset to manage and process data efficiently.\n- **transformers**: A library by Hugging Face that provides pre-trained models and tools for natural language processing. We will use `AutoTokenizer` from this library to tokenize our input text.","metadata":{"_uuid":"a8824ae5-278c-4270-abfc-0d1078c0028b","_cell_guid":"c203cd4e-349b-428c-b8d4-50f5fe868351","id":"TbtDtHvB6as2","trusted":true}},{"cell_type":"code","source":"import math\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"_uuid":"11b0ec28-7ec6-42d2-a17b-0f392c360fba","_cell_guid":"68325831-be62-4483-9894-bce86f1d7d35","collapsed":false,"id":"uRy7FCqznlY1","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. Input Embeddings**\n\nThis cell initializes the embedding layer that converts tokenized input into dense numerical vectors. Embeddings capture semantic relationships between tokens and are crucial for the Transformer model to understand the meaning of words in the input sequence.\n\nAn embedding layer is essentially a lookup table where each word in the vocabulary is mapped to a vector of fixed size (embedding dimension). These vectors are learned during training and represent words in a continuous vector space. By the end of training, words with similar meanings have similar vector representations. \n\nIn the context of natural language processing, vectors are mathematical representations of words. Each word is represented by a point in a high-dimensional space. The dimensions can be thought of as features that capture different aspects of the word's meaning. \n\nFor example, in a 3-dimensional space, a word like \"king\" might be represented by a vector (0.5, 0.2, 0.7). These vectors allow us to perform mathematical operations to understand word relationships. One common operation is calculating the cosine similarity between two vectors to measure how similar they are. Cosine similarity is the cosine of the angle between two vectors, with values ranging from -1 (completely opposite) to 1 (exactly the same).","metadata":{"_uuid":"8067d30a-a92e-4326-8ffc-db34256eefc4","_cell_guid":"468082ac-473e-430b-aef8-d752b3fd8494","id":"SppqCbVkwUvs","trusted":true}},{"cell_type":"code","source":"class InputEmbedding(nn.Module):\n    def __init__(self, embed_dim: int, vocab_size: int):\n        \"\"\"\n        Initialize the InputEmbedding module.\n\n        Args:\n            embed_dim (int): The dimensionality of the input embedding.\n            vocab_size (int): The size of the vocabulary.\n\n        \"\"\"\n        super().__init__()\n        # Store the dimensionality and vocabulary size\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n\n        # Create an embedding layer that maps the vocabulary to an embed_dim-dimensional space\n        # The embedding layer should have shape (vocab_size, embed_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Perform the forward pass of the InputEmbedding module.\n\n        Args:\n            x (tensor): The input tensor.\n\n        Returns:\n            tensor: The embedded input tensor after scaling it by the square root of the dimensionality.\n\n        \"\"\"\n        # Embed the input tensor using the embedding layer\n        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n        embedded_input = self.embedding(x)\n        # Scale the embedded input tensor by the square root of the dimensionality\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        scaled_embedded_input = embedded_input * torch.sqrt(torch.tensor(self.embed_dim))\n        return scaled_embedded_input","metadata":{"_uuid":"c09631bb-64a9-4e68-916a-832ed9c53f33","_cell_guid":"afc3c357-5273-4b69-a550-8f11454aab08","collapsed":false,"id":"CO1r2FtaoRCB","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Positional Encoding**\n\nThis cell computes positional encodings that add information about the order of tokens in the sequence. Positional encodings are necessary because Transformers don't inherently understand the sequential nature of data like RNNs; these encodings help maintain sequential information.\n\nUnlike RNNs, which process input tokens sequentially, Transformers process all tokens in parallel. This parallel processing does not inherently capture the order of tokens. Positional encoding addresses this by adding a unique positional signal to each token, allowing the Transformer to differentiate between tokens at different positions.\n\nThe positional encoding is calculated using sine and cosine functions of different frequencies. This ensures that each position has a unique encoding that can be added to the input embeddings. The following code shows how to implement positional encoding:","metadata":{"_uuid":"0988c67e-2106-48d4-9c44-c7b2de34cfb4","_cell_guid":"8aa3d352-128c-4281-abd4-3cc3d986cd0f","id":"cvlS2mFoyJxg","trusted":true}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim: int = 512, max_seq_len: int = 100, dropout: float = 0.1,):\n        \"\"\"Initialize the PositionalEncoding module.\"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        self.dropout = nn.Dropout(dropout)\n        # Precompute the positional encoding matrix\n        self.positional_encoding = self._precompute_positional_encoding(max_seq_len, embed_dim)\n\n    def _precompute_positional_encoding(self, max_seq_len, embed_dim):\n        \"\"\"Precompute the positional encoding matrix.\"\"\"\n        with torch.no_grad():\n            # Create a positional encoding matrix of shape (max_seq_len, embed_dim)\n            positional_encoding = torch.zeros(max_seq_len, embed_dim)\n            # Create a tensor 'pos' with values [0, 1, 2, ..., max_seq_len - 1] (max_seq_len, 1)\n            position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n            # Compute the positional encoding matrix\n            division_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n            positional_encoding[:, 0::2] = torch.sin(position * division_term)\n            positional_encoding[:, 1::2] = torch.cos(position * division_term)\n            # Shape (max_seq_len, embed_dim) -> (1, max_seq_len, embed_dim)\n            positional_encoding = positional_encoding.unsqueeze(0)\n\n        return positional_encoding\n\n    def forward(self, x):\n        \"\"\"Perform the forward pass of the PositionalEncoding module.\"\"\"\n        # Add the positional encoding matrix to the input tensor\n        x = x + self.positional_encoding[:, : x.size(1)].to(x.device)\n        # Apply dropout to the input tensor\n        x = self.dropout(x)\n        return x","metadata":{"_uuid":"76f27d2b-b026-43fd-a98e-09d05cfcfa66","_cell_guid":"cdfae275-9515-4d85-829f-db2efe144895","collapsed":false,"id":"aJrkmLCboUdS","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Layer Normalization**\n\nImplements layer normalization, which normalizes activations across the features. This improves training stability by reducing the internal covariate shift problem, allowing each layer of the model to learn more independently of others.\n\nLayer normalization is a technique to normalize the inputs of a layer to have zero mean and unit variance. This helps in stabilizing and speeding up the training process. Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the features. This makes it suitable for sequence models where batch statistics can vary significantly.","metadata":{"_uuid":"72164d9d-1541-44e6-89f4-f264e1d77834","_cell_guid":"9a605778-c6e8-406e-adc8-dd166d2ec8f3","id":"EoVCS1LiyZ57","trusted":true}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, embed_dim: int, eps: float = 1e-6):\n        \"\"\"Initialize the LayerNormalization module.\"\"\"\n        super().__init__()\n        self.eps = eps\n        # Create two learnable parameters to scale and shift the normalized input\n        self.gain = nn.Parameter(torch.Tensor(embed_dim).uniform_())  # Initialize with values sampled from a uniform distribution\n        self.bias = nn.Parameter(torch.Tensor(embed_dim).normal_())    # Initialize with values sampled from a normal distribution\n\n\n    def forward(self, x):\n        \"\"\"Perform the forward pass of the LayerNormalization module.\"\"\"\n        # Compute the mean and standard deviation of the input tensor\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        # Zero center by subtracting the mean from the input tensor\n        # Normalize scale by dividing by the standard deviation and add epsilon for numerical stability\n        # Scale and shift the normalized input using the learnable parameters\n        return (x - mean) / (std + self.eps) * self.gain + self.bias","metadata":{"_uuid":"cb22fd1f-99f2-4f70-aad1-4c39fe06d348","_cell_guid":"c36a9a7f-f67f-4dde-a18d-2fd220ea9ced","collapsed":false,"id":"xROh0AlIoXxy","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Feed Forward Block**\n\nDefines a feedforward neural network block, which consists of two linear transformations with a ReLU activation function in between. This block facilitates nonlinear transformations of feature representations learned by the Transformer model.\n\nA feedforward block is a simple fully connected neural network applied to each position separately and identically. It consists of two linear transformations with a ReLU activation function in between. The first linear layer projects the input to a higher-dimensional space, and the second linear layer projects it back to the original space. ReLU (Rectified Linear Unit) is an activation function that adds non-linearity to the model, allowing it to learn more complex patterns.","metadata":{"_uuid":"b3256291-6bd3-4baa-b71d-01a99d24bda3","_cell_guid":"8ccdd60b-b099-450e-9fb8-c8be0b7da70d","id":"AqVxXz-_yfu6","trusted":true}},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n    def __init__(self, embed_dim: int, intermediate_size: int, dropout: float = 0.1):\n        \"\"\"Initialize the FeedForwardBlock module.\n        embed_dim is the hidden size of the transformer model functions as input and output size of the FeedForwardBlock\n        intermediate_size is the hidden size of the intermediate layer in the FeedForwardBlock\n        dropout is the dropout probability\n        \"\"\"\n        super().__init__()\n        # embed_dim is the dimensionality of the input and output of the FeedForwardBlock\n        # intermediate_size is the dimensionality of the intermediate layer in the FeedForwardBlock\n        self.fc1 = nn.Linear(embed_dim, intermediate_size) # W1 and B1 in the formula\n        self.fc2 = nn.Linear(intermediate_size, embed_dim) # W2 and B2 in the formula\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"Perform the forward pass of the FeedForwardBlock module.\"\"\"\n        # (Batch, Seq_len, embed_dim) -> (Batch, Seq_len, intermediate_size) -> (Batch, Seq_len, embed_dim)\n        x_intermediate = self.dropout(F.relu(self.fc1(x)))\n        x_output = self.fc2(x_intermediate)\n        return x_output","metadata":{"_uuid":"64afb5cd-2167-44fc-afa4-54caaea77c68","_cell_guid":"302f9a31-2007-4db1-b529-d46d3b56a6e5","collapsed":false,"id":"w5AAMcrloZPj","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. Multi-Head Attention Block**\n\nImplements multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This mechanism enhances the model's ability to capture dependencies and relationships in the input data.\n\nAttention mechanisms allow models to focus on specific parts of the input sequence, giving more importance to certain words or tokens. Multi-head attention extends this idea by having multiple attention heads, each attending to different parts of the sequence independently. The results from each head are then concatenated and linearly transformed.\n\nHere's how it works in detail:\n- **Query, Key, Value (QKV) Transformations**: For each attention head, we have three learned matrices: query (Q), key (K), and value (V). These matrices transform the input into query, key, and value vectors.\n- **Attention Scores**: The attention score is calculated by taking the dot product of the query and key vectors, scaled by the square root of the dimension of the key vectors. This is followed by a softmax operation to obtain the attention weights.\n- **Weighted Sum**: The attention weights are used to compute a weighted sum of the value vectors.\n- **Concatenation and Projection**: The outputs of all the heads are concatenated and projected back to the original dimension.","metadata":{"_uuid":"754921a8-91f2-4558-94f0-eb191fac452f","_cell_guid":"4c54021b-232a-4b80-96d2-5c57b18af6ce","id":"8LODeG5Clvni","trusted":true}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim: int = 512, num_heads: int = 8, attn_dropout: float = 0.1, ff_dropout: float = 0.1, max_len: int = 512):\n        super().__init__()\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.proj_dropout = nn.Dropout(ff_dropout)\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.size()\n        # Apply linear transformations to the input tensor\n        # Take input tensor and apply linear transformation,\n        # then split the tensor into num_heads and head_dim\n        # transpose the tensor into correct order\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, num_heads, head_dim) ->\n        # (batch_size, seq_len, num_heads, head_dim) -> (batch_size, num_heads, seq_len, head_dim)\n        q = self.query(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        k = self.key(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n        v = self.value(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n\n        # Compute attention scores using Einsum\n        # b: batch size, h: num_heads, i: seq_len, j: seq_len, d: head_dim\n        # Multiply query and key tensors element-wise and sum along the shared dimension (head_dim)\n        # Divide by the square root of the dimension of the query/key vectors\n        # Equivalent to: attention = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n        # Shape: (batch_size, num_heads, seq_len, head_dim) * (batch_size, num_heads, seq_len, head_dim)\n        # -> (batch_size, num_heads, seq_len, seq_len)\n        attention = torch.einsum('bhid,bhjd->bhij', q, k) / math.sqrt(q.size(-1))\n\n        # Apply mask if provided\n        if mask is not None:\n            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n\n        # Apply softmax and dropout\n        # Shape: (batch_size, num_heads, seq_len, seq_len) -> (batch_size, num_heads, seq_len, head_dim)\n        attention = self.attn_dropout(F.softmax(attention, dim=-1))\n\n        # Compute the weighted sum of values using attention scores\n        # Equivalent to: torch.matmul(attention, v)\n        # Shape: (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n        # -> (batch_size, num_heads, seq_len, head_dim)\n        y = torch.einsum('bhij,bhjd->bhid', attention, v)\n\n        # Merge the num_heads and head_dim back to the embed_dim\n        # Transpose sequence length and num_heads\n        # Flatten out the full tensor\n        # Reshape based on batch size, sequence length and embed_dim\n        # Shape: (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, num_heads, head_dim)\n        # -> (batch_size, seq_len, num_heads * head_dim)\n        # -> (batch_size, seq_len, embed_dim)\n        y = y.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n\n        # Apply linear transformation and dropout\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        return self.proj_dropout(self.proj(y))","metadata":{"_uuid":"82947d60-607a-45e9-87bd-557e20b64c46","_cell_guid":"62ae2416-c110-4cf8-8d7a-77c78fa3cb63","collapsed":false,"id":"Y1QgdrDKouit","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. Residual Connection**\n\nDefines a residual connection that adds the input to the output of a sub-layer before applying layer normalization. This helps in mitigating the vanishing gradient problem and allows gradients to flow more effectively during training.\n\nResidual connections, also known as skip connections, allow the input to bypass one or more layers. By adding the input directly to the output of a layer, we help preserve the original signal and prevent the gradients from vanishing or exploding during backpropagation. This makes it easier to train deep networks.\n\nImplementation time :)","metadata":{"_uuid":"db252541-44b6-4fd1-b2bc-ad63d3127f47","_cell_guid":"576a07e6-b481-4642-81f9-01a62b2c96f5","id":"SG-O99lzzFGD","trusted":true}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, embed_dim, dropout: float = 0.1):\n        \"\"\"Initialize the ResidualConnection module.\"\"\"\n        super().__init__()\n        self.layer_norm = LayerNormalization(embed_dim=embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"\"\"Perform the forward pass of the ResidualConnection module.\"\"\"\n        # Apply sublayer (e.g., feedforward block)\n        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        sublayer_output = sublayer(x)\n        # Apply layer normalization\n        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        normalized_x = self.layer_norm(x)\n        # Add residual connection\n        # (batch_size, seq_len, embed_dim) + (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        residual_output = normalized_x + sublayer_output\n        # Apply dropout to the sum\n        return self.dropout(residual_output)","metadata":{"_uuid":"a2ce9de6-3864-4784-9a8e-95c861647969","_cell_guid":"4c6cbaa4-d945-4380-bf6e-8c0a51f2f1fd","collapsed":false,"id":"G8cg1ZanowOw","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**7. Projection Head**\n\nDefines the projection head, which projects the model's hidden representations back into the original vocabulary space for output generation. This final layer ensures that the model's output is interpretable in terms of the input tokens.\n\nThe projection head is a linear layer that maps the high-dimensional hidden states of the model back to the vocabulary space. This allows the model to generate probabilities for each token in the vocabulary, enabling tasks like text generation and classification.","metadata":{"_uuid":"1f310812-3b2b-4826-8de2-05346e50411e","_cell_guid":"af622f08-f4de-450d-9b3c-460f0c1d364f","id":"lDeMGfZ4zOpz","trusted":true}},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(self, embed_dim: int, vocab_size: int):\n        \"\"\"Initialize the ProjectionHead module.\"\"\"\n        super().__init__()\n        self.fc = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        \"\"\"Perform the forward pass of the ProjectionHead module.\"\"\"\n        # Apply linear transformation to the input tensor\n        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, vocab_size)\n        return self.fc(x)","metadata":{"_uuid":"11584293-7790-46ed-8336-e2c9ced9817c","_cell_guid":"9c8af97a-8a58-4035-bd67-28139ef434ed","collapsed":false,"id":"fj8rp6cLo2RM","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**8. Transformer Block**\n\nCombines all the previously defined components (multi-head attention, feedforward block, residual connection, and layer normalization) into a single Transformer block. This block forms the core building block of the Transformer architecture, facilitating the model's ability to process and generate text.\n\nA Transformer block consists of two main sub-layers:\n- **Multi-Head Attention**: This sub-layer helps the model focus on different parts of the input sequence simultaneously.\n- **Feedforward Neural Network**: This sub-layer performs nonlinear transformations on each position separately and identically.\n\nEach sub-layer is followed by a residual connection and layer normalization to ensure stable training. The following code shows how to implement a Transformer block:","metadata":{"_uuid":"132625de-58e1-4f15-8f3b-6a78705850d7","_cell_guid":"23aecc3d-a00d-44cb-9e8f-b61910fea78a","id":"R1p78gyWw8jo","trusted":true}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int = 512,\n        num_heads: int = 8,\n        ff_dim: int = 2048,\n        attn_dropout: float = 0.1,\n        ff_dropout: float = 0.1,\n        dropout: float = 0.1,\n        max_len: int = 512,\n    ):\n        super().__init__()\n        # Initialize multi-head self-attention mechanism\n        self.MultiHeadAttention = MultiHeadAttention(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            attn_dropout=attn_dropout,\n            ff_dropout=ff_dropout,\n            max_len=max_len,\n            )\n        # Initialize feed-forward block\n        self.feed_forward = FeedForwardBlock(\n            embed_dim=embed_dim,\n            intermediate_size=ff_dim,\n            dropout=ff_dropout,\n            )\n        # Initialize residual connections\n        self.residual_connection1 = ResidualConnection(embed_dim=embed_dim, dropout=dropout)\n        self.residual_connection2 = ResidualConnection(embed_dim=embed_dim, dropout=dropout)\n\n    def forward(self, x, attention_mask=None):\n        # Apply self-attention mechanism with residual connection\n        x_with_attention = self.residual_connection1(x, lambda x: self.MultiHeadAttention(x, mask=attention_mask))\n        # Apply feed-forward block with residual connection\n        x_with_ff = self.residual_connection2(x_with_attention, self.feed_forward)\n        return x_with_ff","metadata":{"_uuid":"aab287dc-63a2-43a6-9356-e40a41135dc6","_cell_guid":"c1f7498b-b21f-4e09-8c2a-f69b220c067c","collapsed":false,"id":"XahbnmqHo3k6","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**9. Building the Transformer**\n\nConstructs the entire Transformer architecture by stacking multiple Transformer blocks. This setup creates a deep neural network capable of processing sequential data and generating output predictions based on learned patterns and relationships.\n\nThe following code shows how to build the Transformer model by stacking several Transformer blocks and adding the input embedding, positional encoding, and projection head:","metadata":{"_uuid":"91b413c5-60d2-409c-bcbf-5c1ffa433274","_cell_guid":"7f4d43c8-44b0-4299-9c1d-91770c6b0a24","id":"pNgYNrR9zaTP","trusted":true}},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        embed_dim: int = 512,\n        max_len: int = 512,\n        embed_dropout: float = 0.1,\n        num_blocks: int = 6,\n        num_heads: int = 8,\n        ff_dim: int = 2048,\n        attn_dropout: float = 0.1,\n        ff_dropout: float = 0.1\n    ):\n        super().__init__()\n        self.max_len = max_len\n        self.token_embedding = InputEmbedding(\n            embed_dim=embed_dim,\n            vocab_size=vocab_size,\n            )\n        self.positional_embedding = PositionalEncoding(\n            embed_dim=embed_dim,\n            max_seq_len=max_len,\n            dropout=embed_dropout,\n            )\n        self.blocks = nn.ModuleList([DecoderBlock(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            ff_dim=ff_dim,\n            attn_dropout=attn_dropout,\n            ff_dropout=ff_dropout,\n            max_len=max_len,\n            ) for _ in range(num_blocks)])\n\n        self.projection_head = ProjectionHead(embed_dim=embed_dim, vocab_size=vocab_size)\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n        # Shape: (batch_size, seq_len) -> (seq_len)\n        seq_len = input_ids.size(1)\n        assert seq_len <= self.max_len, \"Sequence longer than model capacity\"\n\n        # Token embedding\n        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n        x = self.token_embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n\n        # Add positional embedding\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        x = self.positional_embedding(x)\n\n        # Forward through decoder blocks\n        # output of each block is the hidden state of the transformer\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n        for block in self.blocks:\n            x = block(x, attention_mask=attention_mask)\n\n        # Linear layer for output logits\n        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, vocab_size)\n        x = self.projection_head(x)  # (batch_size, seq_len, vocab_size)\n\n        return x","metadata":{"_uuid":"b978d598-79e8-43f5-b166-9153aa99fa7f","_cell_guid":"ed6dcdc0-cad7-4029-84b8-a1129f3af38b","collapsed":false,"id":"-Xd1UOAKpA2c","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**10. Sample Usage**\n\nNow we demonstrate how to initialize and use the Transformer model for tasks such as text generation and prediction. This cell provides a practical example of how to interact with the Transformer architecture once it's been constructed.\n\nThe following code initializes the Transformer model with specified parameters and demonstrates how to use it for text generation:","metadata":{"_uuid":"738085af-e639-4f04-9fdc-6e5f2606b8c1","_cell_guid":"f0fdd6ff-823c-4e08-bdfd-07d290af0d88","id":"Mz61S0D1zgJr","trusted":true}},{"cell_type":"code","source":"# Define model parameters\nvocab_size = 50257  # Example vocab size; specific to GPT2 tokenizer (borrowed from OpenAI :) )\nembed_dim = 768\nmax_len = 1024 # This can be adjusted based on the use case\nembed_dropout = 0.1\nnum_blocks = 6 # This can be adjusted based on the use case\nnum_heads = 8 # This can be adjusted based on the use case\nff_dim = 2048 # This can be adjusted based on the use case\nattn_dropout = 0.1\nff_dropout = 0.1\n\n# Initialize GPT model\nmodel = GPT(\n    vocab_size=vocab_size,\n    embed_dim=embed_dim,\n    max_len=max_len,\n    embed_dropout=embed_dropout,\n    num_blocks=num_blocks,\n    num_heads=num_heads,\n    ff_dim=ff_dim,\n    attn_dropout=attn_dropout,\n    ff_dropout=ff_dropout\n)","metadata":{"_uuid":"c2c31320-c812-4b0b-b1d4-3728e1d2419f","_cell_guid":"d7fe877b-6e42-4b58-a2e1-91b94766226b","collapsed":false,"id":"K3h2icNIpCSG","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**11. Training the Transformer**\n\n**11.1 Data Preprocessing**\n\nData preprocessing is a crucial step in training a Transformer model. It involves preparing the input data in a format suitable for the model. This includes tokenizing the text, padding or truncating sequences to a fixed length, and creating attention masks.\n\nHere's a sample dataset and the code to preprocess it:","metadata":{"_uuid":"b2b15872-e2a1-4b45-ab2e-89ffe63480b9","_cell_guid":"7c3f4e66-3c55-479a-9395-03903df2d172","id":"t63qvcsqzo43","trusted":true}},{"cell_type":"code","source":"sample_data = [\n    \"Mary had a little lamb\",\n    \"Its fleece was white as snow\",\n    \"And everywhere that Mary went\",\n    \"The lamb was sure to go\",\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Artificial Intelligence is transforming the world\",\n    \"PyTorch is a popular deep learning framework\",\n    \"Transformers are powerful models for NLP\",\n    \"Machine learning can solve many complex problems\",\n    \"Deep learning models require large amounts of data\",\n    \"Natural Language Processing is a fascinating field\",\n    \"GPT models are great for generating text\",\n    \"Attention mechanisms help models focus on important parts of the input\",\n    \"Feedforward neural networks are used in many applications\",\n    \"Multi-head attention enhances the model's ability to capture relationships\",\n    \"Positional encoding adds order information to the input\",\n    \"Residual connections help in training deep neural networks\",\n    \"Layer normalization stabilizes the learning process\",\n    \"Recurrent Neural Networks process input sequentially\",\n    \"Convolutional Neural Networks are used for image processing\",\n    \"Autoencoders are used for unsupervised learning tasks\",\n    \"Reinforcement learning involves training agents to make decisions\",\n    \"Generative Adversarial Networks generate new data samples\",\n    \"Transfer learning leverages pre-trained models for new tasks\",\n    \"Self-supervised learning uses data itself as a source of supervision\",\n    \"The sigmoid function is an activation function used in neural networks\",\n    \"The softmax function converts logits to probabilities\",\n    \"Backpropagation is the algorithm used for training neural networks\",\n    \"Optimization algorithms like Adam and SGD are used to train models\",\n    \"Loss functions measure the error in model predictions\",\n    \"Gradient descent is an optimization technique used in machine learning\",\n    \"Hyperparameter tuning is crucial for model performance\",\n    \"Data augmentation techniques are used to increase dataset size\",\n    \"Overfitting occurs when a model performs well on training data but poorly on new data\",\n    \"Regularization techniques help prevent overfitting\",\n    \"Cross-validation is used to evaluate model performance\",\n    \"Batch normalization improves training speed and stability\",\n    \"Dropout is a regularization technique to prevent overfitting\",\n    \"Long Short-Term Memory networks are used for sequential data\",\n    \"Bidirectional RNNs process data in both forward and backward directions\",\n    \"Sequence-to-sequence models are used for tasks like translation\",\n    \"Beam search is used for decoding sequences in NLP\",\n    \"Named Entity Recognition identifies entities in text\",\n    \"Part-of-Speech tagging assigns grammatical categories to words\",\n    \"Dependency parsing analyzes the grammatical structure of a sentence\",\n    \"Word embeddings capture semantic meaning of words\",\n    \"Sentence embeddings represent entire sentences as vectors\",\n    \"Graph Neural Networks are used for processing graph-structured data\",\n    \"Knowledge graphs represent information in a structured form\",\n    \"Collaborative filtering is used in recommendation systems\",\n    \"Content-based filtering recommends items based on item features\",\n    \"Matrix factorization techniques are used in recommendation systems\",\n    \"Anomaly detection identifies unusual patterns in data\",\n    \"Time series forecasting predicts future values based on past data\",\n    \"Clustering algorithms group similar data points together\",\n    \"Dimensionality reduction techniques like PCA reduce data complexity\",\n    \"t-SNE is a technique for visualizing high-dimensional data\",\n    \"UMAP is another technique for visualizing high-dimensional data\",\n    \"Bayesian networks represent probabilistic relationships among variables\",\n    \"Markov chains model systems that transition from one state to another\",\n    \"Hidden Markov Models are used for sequence modeling\",\n    \"Support Vector Machines are used for classification tasks\",\n    \"Decision trees are used for both classification and regression\",\n    \"Random forests are ensembles of decision trees\",\n    \"Gradient boosting combines weak learners to create a strong learner\",\n    \"XGBoost is a popular implementation of gradient boosting\",\n    \"LightGBM is another gradient boosting framework\",\n    \"CatBoost is a gradient boosting framework designed for categorical features\",\n    \"K-means is a popular clustering algorithm\",\n    \"Hierarchical clustering builds a hierarchy of clusters\",\n    \"DBSCAN is a density-based clustering algorithm\",\n    \"Linear regression models the relationship between two variables\",\n    \"Logistic regression is used for binary classification\",\n    \"Ridge regression adds regularization to linear regression\",\n    \"Lasso regression adds L1 regularization to linear regression\",\n    \"Elastic net combines L1 and L2 regularization\",\n    \"Principal Component Analysis is used for dimensionality reduction\",\n    \"Independent Component Analysis separates a multivariate signal into additive components\",\n    \"Factor analysis models observed variables as linear combinations of potential factors\",\n    \"Canonical correlation analysis explores the relationships between two sets of variables\",\n    \"Discriminant analysis is used for classification\",\n    \"Quadratic discriminant analysis extends linear discriminant analysis\",\n    \"Naive Bayes classifiers are based on Bayes' theorem\",\n    \"Gaussian Naive Bayes assumes the features follow a normal distribution\",\n    \"Multinomial Naive Bayes is used for discrete data\",\n    \"Bernoulli Naive Bayes is used for binary/boolean data\",\n    \"Markov Decision Processes are used for decision making in stochastic environments\",\n    \"Q-learning is a reinforcement learning algorithm\",\n    \"Policy gradients optimize the policy directly in reinforcement learning\",\n    \"Deep Q-Networks use neural networks to approximate Q-values\",\n    \"Actor-critic methods combine policy gradients and value function approximation\",\n    \"Proximal Policy Optimization is a popular reinforcement learning algorithm\",\n    \"AlphaGo used deep reinforcement learning to play Go\",\n    \"Game theory studies strategic interactions between agents\",\n    \"Mechanism design is a field related to game theory\",\n    \"Econometrics applies statistical methods to economic data\",\n    \"Time series analysis involves analyzing data points collected over time\",\n    \"Survival analysis studies the time until an event occurs\",\n    \"Epidemiology models the spread of diseases\",\n    \"Bioinformatics applies computational techniques to biological data\",\n    \"Genomics studies the structure, function, evolution, and mapping of genomes\",\n    \"Proteomics studies the structure and function of proteins\",\n    \"Metabolomics studies the chemical processes involving metabolites\",\n    \"Systems biology models complex biological systems\",\n    \"Synthetic biology designs and constructs new biological parts and systems\",\n    \"Structural biology studies the molecular structure of biological macromolecules\",\n    \"Quantum computing uses principles of quantum mechanics for computation\",\n    \"Quantum machine learning applies quantum computing to machine learning\",\n    \"Cryptography secures communication in the presence of adversaries\",\n    \"Blockchain is a decentralized ledger technology\",\n    \"Smart contracts are self-executing contracts with the terms directly written into code\",\n    \"Internet of Things connects everyday objects to the internet\",\n    \"Edge computing processes data at the edge of the network\",\n    \"Fog computing extends cloud computing to the edge of the network\",\n    \"5G networks provide high-speed wireless communication\",\n    \"Software-defined networking decouples the control and data planes in networking\",\n    \"Network function virtualization virtualizes network services\",\n    \"Cybersecurity protects computer systems from theft or damage\",\n    \"Penetration testing evaluates the security of a computer system\",\n    \"Intrusion detection systems monitor networks for malicious activity\",\n    \"Firewalls control incoming and outgoing network traffic\",\n    \"Public key infrastructure manages digital keys and certificates\",\n    \"Zero-trust architecture assumes no trusted zones in a network\",\n    \"Digital forensics investigates digital evidence\",\n    \"Data governance ensures data is managed and used responsibly\",\n    \"Data privacy protects personal data from unauthorized access\",\n    \"Data anonymization removes personally identifiable information\",\n    \"Data integrity ensures data is accurate and consistent\",\n    \"Data wrangling involves transforming raw data into a usable format\",\n    \"Data visualization presents data in a visual context\",\n    \"Data storytelling combines data visualization with narrative techniques\",\n    \"Business intelligence analyzes data to support business decision-making\",\n    \"Customer relationship management manages interactions with customers\",\n    \"Enterprise resource planning integrates business processes\",\n    \"Supply chain management oversees the flow of goods and services\",\n    \"Project management plans and organizes resources to achieve specific goals\",\n    \"Agile development is an iterative approach to software development\",\n    \"Scrum is a framework for agile project management\",\n    \"Kanban is a visual system for managing work\",\n    \"Lean methodology focuses on maximizing value while minimizing waste\",\n    \"DevOps combines software development and IT operations\",\n    \"Continuous integration automates the integration of code changes\",\n    \"Continuous delivery automates the delivery of software updates\",\n    \"Microservices architecture structures an application as a collection of loosely coupled services\",\n    \"Containerization packages software into containers\",\n    \"Docker is a platform for developing, shipping, and running applications in containers\",\n    \"Kubernetes orchestrates containerized applications\",\n    \"Serverless computing allows developers to build applications without managing servers\",\n    \"Function as a Service runs code in response to events\",\n    \"Platform as a Service provides a platform for developing, running, and managing applications\",\n    \"Infrastructure as a Service provides virtualized computing resources over the internet\",\n    \"Software as a Service delivers software over the internet\",\n    \"Cloud computing delivers computing services over the internet\",\n    \"Hybrid cloud combines public and private clouds\",\n    \"Multi-cloud uses multiple cloud services from different providers\",\n    \"Big data analytics analyzes large and complex datasets\",\n    \"Hadoop is a framework for processing large datasets\",\n    \"Spark is a fast and general-purpose cluster computing system\",\n    \"NoSQL databases are designed for large-scale data storage and retrieval\",\n    \"Graph databases store data in graph structures\",\n    \"Document databases store data in document format\",\n    \"Key-value stores are a simple type of NoSQL database\",\n    \"Column-family stores organize data into columns\",\n    \"Data lakes store raw data in its native format\",\n    \"Data warehouses store structured data for analysis\",\n    \"ETL processes extract, transform, and load data\",\n    \"Data pipelines automate the flow of data\",\n    \"Machine learning pipelines automate the process of training and deploying models\",\n    \"Feature engineering creates new features from raw data\",\n    \"Feature selection selects the most important features for a model\",\n    \"Model evaluation assesses the performance of a model\",\n    \"Model interpretability explains how a model makes decisions\",\n    \"Fairness in machine learning ensures models do not discriminate against certain groups\",\n    \"Ethics in AI addresses the moral implications of AI\",\n    \"Explainable AI makes AI decisions understandable to humans\",\n    \"Human-in-the-loop AI involves humans in the AI decision-making process\",\n    \"AI for social good applies AI to address social challenges\",\n    \"AI policy and governance address the regulation and management of AI\",\n    \"AI safety ensures AI systems operate as intended\",\n    \"AI alignment aligns AI systems with human values\",\n    \"AI ethics examines the ethical implications of AI\",\n    \"AI and law explores the legal aspects of AI\",\n    \"AI in healthcare applies AI to medical and healthcare tasks\",\n    \"AI in finance applies AI to financial tasks\",\n    \"AI in education applies AI to learning and teaching\",\n    \"AI in transportation applies AI to transportation systems\",\n    \"AI in robotics integrates AI with robotics\",\n    \"AI in gaming enhances game design and player experience\",\n    \"AI in creative arts enhances creativity in art, music, and literature\",\n    \"AI in agriculture optimizes farming practices\",\n    \"AI in environmental science addresses environmental challenges\",\n    \"AI in space exploration advances space missions\",\n    \"AI in manufacturing optimizes production processes\",\n    \"AI in customer service improves customer interactions\",\n    \"AI in marketing personalizes marketing campaigns\",\n    \"AI in sales improves sales processes\",\n    \"AI in human resources enhances recruitment and employee management\",\n    \"AI in legal services assists with legal research and case management\",\n    \"AI in public services improves government operations\",\n    \"AI in security enhances threat detection and response\",\n    \"AI in smart cities optimizes urban infrastructure\",\n    \"AI in energy optimizes energy production and consumption\",\n    \"AI in retail enhances the shopping experience\",\n    \"AI in fashion designs new clothing and accessories\",\n    \"AI in sports analyzes player performance\",\n    \"AI in entertainment creates new content\",\n    \"AI in hospitality improves guest experiences\",\n    \"AI in tourism enhances travel planning\",\n    \"AI in real estate optimizes property management\",\n    \"AI in construction improves building design and management\",\n    \"AI in logistics optimizes supply chain operations\",\n    \"AI in insurance assesses risk and processes claims\",\n    \"AI in telecommunications optimizes network operations\",\n    \"AI in mining improves resource extraction\",\n    \"AI in chemical engineering optimizes chemical processes\",\n    \"AI in pharmaceuticals enhances drug discovery\",\n    \"AI in biotechnology advances biological research\",\n    \"AI in materials science discovers new materials\",\n    \"AI in nanotechnology advances nanoscale research\",\n    \"AI in quantum physics explores quantum phenomena\",\n    \"AI in cognitive science studies human cognition\",\n    \"AI in linguistics analyzes language\",\n    \"AI in psychology studies human behavior\",\n    \"AI in sociology studies social interactions\",\n    \"AI in anthropology studies human cultures\",\n    \"AI in economics analyzes economic data\",\n    \"AI in political science studies political systems\",\n    \"AI in history analyzes historical data\",\n    \"AI in philosophy explores philosophical questions\",\n    \"AI in theology studies religious texts\",\n    \"AI in archaeology analyzes archaeological data\",\n    \"AI in education policy studies education systems\",\n    \"AI in public health analyzes health data\",\n    \"AI in food science optimizes food production\",\n    \"AI in animal science studies animal behavior\",\n    \"AI in veterinary medicine advances animal healthcare\",\n    \"AI in plant science studies plant growth\",\n    \"AI in forestry manages forest resources\",\n    \"AI in marine biology studies ocean life\",\n    \"AI in meteorology predicts weather patterns\",\n    \"AI in climatology studies climate change\",\n    \"AI in geology studies Earth's structure\",\n    \"AI in astronomy explores the universe\",\n    \"AI in aerospace advances space technology\",\n    \"AI in automotive industry designs smart vehicles\",\n    \"AI in electrical engineering optimizes electronic systems\",\n    \"AI in mechanical engineering designs mechanical systems\",\n    \"AI in civil engineering improves infrastructure\",\n    \"AI in chemical engineering optimizes chemical processes\",\n    \"AI in industrial engineering improves manufacturing\",\n    \"AI in biomedical engineering develops medical devices\",\n    \"AI in environmental engineering solves environmental problems\",\n    \"AI in urban planning designs better cities\",\n    \"AI in architecture designs smarter buildings\",\n    \"AI in interior design creates personalized spaces\",\n    \"AI in product design innovates new products\",\n    \"AI in graphic design creates digital art\",\n    \"AI in photography enhances photo editing\",\n    \"AI in videography improves video production\",\n    \"AI in music composition creates new music\",\n    \"AI in sound engineering optimizes audio production\",\n    \"AI in theater designs better performances\",\n    \"AI in film industry enhances movie production\",\n    \"AI in animation creates animated content\",\n    \"AI in virtual reality creates immersive experiences\",\n    \"AI in augmented reality enhances real-world experiences\",\n    \"AI in mixed reality combines virtual and real worlds\",\n    \"AI in gaming industry improves game design\",\n    \"AI in fitness optimizes workout routines\",\n    \"AI in wellness enhances mental health\",\n    \"AI in mindfulness promotes relaxation\",\n    \"AI in meditation guides meditation practices\",\n    \"AI in nutrition optimizes diet plans\",\n    \"AI in personal finance manages finances\",\n    \"AI in investment advises on investments\",\n    \"AI in retirement planning plans for retirement\",\n    \"AI in estate planning manages estates\",\n    \"AI in tax planning optimizes taxes\",\n    \"AI in wealth management grows wealth\",\n    \"AI in philanthropy optimizes charitable giving\",\n    \"AI in community service enhances community projects\",\n    \"AI in nonprofit management manages nonprofits\",\n    \"AI in humanitarian aid improves disaster response\",\n    \"AI in conflict resolution mediates disputes\",\n    \"AI in international relations studies global interactions\",\n    \"AI in diplomacy enhances diplomatic efforts\",\n    \"AI in intelligence analyzes security threats\",\n    \"AI in military applications advances defense technology\",\n    \"AI in space missions advances space exploration\",\n    \"AI in planetary science studies planets\",\n    \"AI in astrophysics explores cosmic phenomena\",\n    \"AI in cosmology studies the universe\",\n    \"AI in astrobiology searches for extraterrestrial life\",\n    \"AI in space weather predicts space weather events\",\n    \"AI in satellite technology enhances satellite operations\",\n    \"AI in space debris management addresses space debris\",\n    \"AI in space mining explores space resources\",\n    \"AI in space tourism develops space tourism\",\n    \"AI in astronaut training trains astronauts\",\n    \"AI in space habitat designs space habitats\",\n    \"AI in space station operations manages space stations\",\n    \"AI in planetary defense protects Earth from asteroids\",\n    \"AI in space policy studies space laws\",\n    \"AI in space law regulates space activities\",\n    \"AI in space economics studies space economy\",\n    \"AI in space medicine advances space healthcare\",\n    \"AI in space agriculture grows food in space\",\n    \"AI in space exploration explores space\",\n    \"AI in space robotics develops space robots\",\n    \"AI in space navigation navigates space missions\",\n    \"AI in space communication enhances space communication\",\n    \"AI in space telemetry monitors space missions\",\n    \"AI in space data analysis analyzes space data\",\n    \"AI in space research advances space knowledge\",\n    \"AI in space innovation innovates space technology\",\n    \"AI in space sustainability promotes space sustainability\",\n    \"AI in space ethics studies space ethics\",\n    \"AI in space culture explores space culture\",\n    \"AI in space history studies space history\",\n    \"AI in space future studies future space missions\",\n    \"AI in space colonization explores space colonization\",\n    \"AI in space economy studies space economy\",\n    \"AI in space governance regulates space activities\",\n    \"AI in space cooperation promotes space cooperation\",\n    \"AI in space competition analyzes space competition\",\n    \"AI in space strategy plans space missions\",\n    \"AI in space missions manages space missions\",\n    \"AI in space education educates about space\",\n    \"AI in space advocacy advocates for space\",\n    \"AI in space outreach reaches out about space\",\n    \"AI in space engagement engages people in space\",\n    \"AI in space awareness raises space awareness\",\n    \"AI in space inspiration inspires about space\",\n    \"AI in space creativity creates space content\",\n    \"AI in space communication communicates about space\",\n    \"AI in space storytelling tells space stories\",\n    \"AI in space visualization visualizes space\",\n    \"AI in space simulation simulates space missions\",\n    \"AI in space exploration explores new frontiers\",\n    \"AI in space colonization colonizes new worlds\",\n    \"AI in space governance establishes space governance\",\n    \"AI in space innovation drives space innovation\",\n    \"AI in space sustainability ensures space sustainability\",\n    \"AI in space ethics promotes space ethics\",\n    \"AI in space culture celebrates space culture\",\n    \"AI in space history preserves space history\",\n    \"AI in space future envisions space future\",\n]","metadata":{"_uuid":"a35c15dc-d9be-4b83-ab84-92648d78a031","_cell_guid":"453cb7a5-12f4-42e6-96f5-603ce6352bcd","collapsed":false,"id":"RXq_8wNfpI3w","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GPTDataset(Dataset):\n    def __init__(self, data:list, tokenizer, max_length:int):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.end_token = tokenizer.eos_token_id\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data[idx]\n        input_txt = self.tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n        text_len = input_txt.size(0)\n        if text_len < self.max_length:\n            padding_len = self.max_length - text_len\n            padding = torch.tensor([self.end_token] * padding_len)\n            input_ids = torch.cat((input_txt, padding), dim=0)\n            label = torch.cat((input_txt[1:], torch.tensor([self.end_token]), padding), dim=0)\n        else:\n            input_ids = input_txt[:self.max_length]\n            label = torch.cat((input_txt[1:self.max_length], torch.tensor([self.end_token])), dim=0)\n        return input_ids, label","metadata":{"_uuid":"94dbc934-104e-4c2b-9d33-ebfee273e655","_cell_guid":"9852b78d-c888-4c44-aee2-586e5e4588ca","collapsed":false,"id":"K0gnEDjPpLbl","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ntrain_dataset = GPTDataset(\n    data = sample_data,\n    tokenizer = tokenizer,\n    max_length = 200,\n    )","metadata":{"_uuid":"eddde9c2-6ec5-48da-9448-189d1c20e6c0","_cell_guid":"1e4a38aa-de0a-4690-a7d7-d9a34857536c","collapsed":false,"id":"dTv_X60kpRnu","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, label = train_dataset[2]\ninput_ids = input_ids.unsqueeze(0)\nlabel = label.unsqueeze(0)\n\nprint(\"Label:\", label)\nprint(\"Input IDs:\", input_ids)\n\nprint(\"Label Shape:\", label.shape)\nprint(\"Input IDs Shape:\", input_ids.shape)","metadata":{"_uuid":"ec40513f-39fb-4e07-990a-532dfe9140af","_cell_guid":"6a68a8e2-e5de-4178-b2d6-8e87efcfa4e8","collapsed":false,"id":"aWjzHszMpVBP","outputId":"bf68650e-26cc-4473-d3dd-025647e2fb11","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**11.2 Model Training**\n\nTraining the Transformer model involves defining a loss function, optimizer, and training loop. The loss function measures how well the model's predictions match the true labels. The optimizer updates the model's parameters to minimize the loss.","metadata":{"_uuid":"8e935ac9-59f3-4d36-8efc-0c131cbdb869","_cell_guid":"9ebafcf1-f3a2-45af-b4ac-7c4621c724ca","id":"nJ8QYo541BPT","trusted":true}},{"cell_type":"markdown","source":"# Generate Square Subsequent Mask\n\nIn Transformer models, especially during training, it's important to ensure that the model doesn't \"cheat\" by looking ahead at the subsequent tokens when generating sequences. To prevent this, we use a mask that blocks future positions. This is called a \"causal\" or \"look-ahead\" mask.\n\nThe `generate_square_subsequent_mask` function creates such a mask. Heres a detailed breakdown of the function and its implementation:\n\n### Function Explanation\n\n**Purpose**: \nThe function generates a square mask for a sequence of a given size. This mask is used to ensure that each position in the sequence can only attend to the positions before it (and itself), and not the positions that come after it. \n\n**Arguments**:\n- `size` (int): The size of the square mask. This is typically the length of the sequence.\n- `device` (torch.device): The device on which the mask is created (e.g., CPU or GPU).\n\n**Returns**:\n- `torch.Tensor`: A mask tensor of shape `(size, size)`, where positions that should be masked are filled with `float('-inf')`, and unmasked positions are filled with `float(0.0)`.\n\n### Code Explanation\n\n1. **Initialization**:\n    ```python\n    mask = torch.triu(torch.ones(size, size, device=device) * float('-inf'), diagonal=1)\n    ```\n    - `torch.ones(size, size, device=device)`: Creates a tensor of shape `(size, size)` filled with ones on the specified device.\n    - `* float('-inf')`: Multiplies all the ones by negative infinity (`-inf`), resulting in a tensor filled with `-inf`.\n    - `torch.triu(..., diagonal=1)`: Extracts the upper triangular part of the tensor, starting from the first diagonal above the main diagonal (hence `diagonal=1`). This effectively sets all elements above the main diagonal to `-inf` and keeps the lower triangle (including the main diagonal) as zeros.\n\n2. **Return Mask**:\n    ```python\n    return mask\n    ```\n    - The function returns the generated mask.\n\n# Example usage\nsize = 5\nmask = generate_square_subsequent_mask(size)\nprint(mask)\n```\n\n### Example Output\n\nFor `size = 5`, the generated mask will look like this:\n\n```\ntensor([[  0., -inf, -inf, -inf, -inf],\n        [  0.,   0., -inf, -inf, -inf],\n        [  0.,   0.,   0., -inf, -inf],\n        [  0.,   0.,   0.,   0., -inf],\n        [  0.,   0.,   0.,   0.,   0.]])\n```\n\nThis mask ensures that each position can only attend to itself and the positions before it, effectively preventing any position from attending to future positions during sequence generation.","metadata":{}},{"cell_type":"code","source":"def generate_square_subsequent_mask(size, device='cpu'):\n    \"\"\"\n    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \n    Args:\n        size (int): The size of the square mask.\n        device (torch.device): The device on which the mask is created.\n    \n    Returns:\n        torch.Tensor: The generated mask tensor of shape (size, size).\n    \"\"\"\n    mask = torch.triu(torch.ones(size, size, device=device) * float('-inf'), diagonal=1)\n    return mask\n\n# Example usage\nsize = 5\nmask = generate_square_subsequent_mask(size)\nprint(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlr = 1e-5\nbatch_size = 2\nnum_epochs = 5","metadata":{"_uuid":"7dbb0139-77e9-440b-b3a4-256b82de6fba","_cell_guid":"b2e284f7-c2d3-44de-b88f-4eb3caf73e9e","collapsed":false,"id":"IGbwsSTUpZcE","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this block of code, we are performing the training loop for our Transformer model. Training a neural network involves multiple epochs, where in each epoch the model sees the entire training dataset once. During each epoch, the dataset is divided into batches to efficiently utilize memory and speed up computations. Heres a detailed explanation of what each part of the code does:\n\n1. **Moving Model to Device**:\n   The model is moved to the specified device (CPU or GPU). Training on a GPU can significantly speed up the process compared to a CPU.\n\n2. **Setting Up Optimizer and Loss Function**:\n   - **Optimizer**: The Adam optimizer is used, which is an adaptive learning rate optimization algorithm that's popular for training deep learning models. It adjusts the learning rate for each parameter dynamically.\n   - **Loss Function**: CrossEntropyLoss is used, which is suitable for classification tasks. It combines LogSoftmax and NLLLoss in one single class, which is useful when training a language model.\n\n3. **Preparing DataLoader**:\n   The DataLoader is used to efficiently handle the dataset during training. It allows us to iterate over the dataset in mini-batches, shuffle the data, and load the data in parallel using multiple workers.\n\n4. **Training Loop**:\n   The loop runs for a specified number of epochs. Each epoch involves one complete pass through the training dataset.\n\n5. **Setting Model to Training Mode**:\n   The model is set to training mode. Certain layers, like dropout, behave differently during training and evaluation, so it's important to set the correct mode.\n\n6. **Initializing Total Loss**:\n   `total_loss` is initialized to accumulate the loss over all batches in the epoch.\n\n7. **Iterating Over Batches**:\n   We iterate over each batch in the DataLoader. Each batch contains a subset of the training data.\n\n8. **Zeroing Gradients**:\n   Before the backward pass, all the gradients for the variables are zeroed. This is important because PyTorch accumulates the gradients on subsequent backward passes.\n\n9. **Unpacking and Moving Data to Device**:\n   The input data and labels are unpacked from the batch and moved to the specified device.\n\n10. **Generating Causal Mask**:\n    A causal mask is generated for the sequence to ensure that each position can only attend to the positions before it (and itself), preventing information leakage from future tokens.\n\n11. **Forward Pass**:\n    A forward pass is performed through the model to obtain the logits (raw predictions before applying softmax).\n\n12. **Flattening Logits and Labels**:\n    The logits and labels are flattened to compute the loss. This is necessary because CrossEntropyLoss expects the inputs to be of shape (N, C) where N is the number of examples and C is the number of classes.\n\n13. **Computing Loss**:\n    The loss between the predicted logits and the true labels is computed using the cross-entropy loss function.\n\n14. **Backward Pass**:\n    A backward pass is performed to compute the gradients of the loss with respect to the model parameters.\n\n15. **Optimizer Step**:\n    The model parameters are updated using the gradients computed during the backward pass.\n\n16. **Accumulating Loss**:\n    The loss is accumulated over all batches to keep track of the total loss for the epoch.\n\n17. **Logging Loss**:\n    At the end of each epoch, the average loss over all batches is printed to monitor the training progress.\n\n### Additional Debugging Lines (Commented Out):\n\n- **Inspecting Input Data and Labels**:\n    These lines are useful for debugging to check if the input data and labels are correctly loaded.\n\n- **Inspecting Logits**:\n    This line helps in inspecting the raw model outputs (logits) to ensure that the model is functioning correctly.\n\n- **Checking for NaN or Inf in Loss**:\n    These lines are useful for checking if the loss contains any NaN or infinite values, which can indicate issues in the training process.\n\n- **Checking for NaN or Inf in Gradients**:\n    These lines check if the gradients contain NaN or infinite values, which can also indicate problems in the training process.\n\nBy following these steps, we train our Transformer model over multiple epochs, updating the model parameters to minimize the loss and improve performance on the training dataset.","metadata":{}},{"cell_type":"code","source":"model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n        # Unpack input and label from the batch and send them to the device\n        input_ids, labels = batch\n        input_ids, labels = input_ids.to(device), labels.to(device)\n\n        # Print input data and labels to inspect\n        # print(f\"Input IDs: {input_ids}\")\n        # print(f\"Labels: {labels}\")\n\n        # Generate the causal mask\n        # Shape: (batch_size, seq_len, seq_len)\n        mask = generate_square_subsequent_mask(input_ids.size(1), device=device)\n\n        # Forward pass\n        logits = model(input_ids=input_ids, attention_mask=mask)\n\n        # Print logits to inspect\n        # print(f\"Logits: {logits}\")\n\n        # Flatten the logits and labels for computing the loss\n        logits_flat = logits.view(-1, logits.size(-1))\n        labels_flat = labels.view(-1)\n\n        # Compute the loss\n        loss = criterion(logits_flat, labels_flat)\n\n        # Check for NaN or Inf values in loss\n        # if torch.isnan(loss) or torch.isinf(loss):\n        #     print(f\"NaN or Inf detected in loss at epoch {epoch}\")\n        #     continue\n\n        # Backward pass and optimization step\n        loss.backward()\n\n        # Check for NaN or Inf values in gradients\n        # for name, param in model.named_parameters():\n        #     if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n        #         print(f\"NaN or Inf detected in gradients at epoch {epoch} for parameter {name}\")\n        #         continue\n\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')","metadata":{"_uuid":"d9e600b4-da71-4d30-8629-ae918d4c11a5","_cell_guid":"70a8bba2-c549-4f3a-8236-f2d8205ddeec","collapsed":false,"id":"lHwFX_z9pcJu","outputId":"11006cd9-be81-4685-aa7c-d02f4a3fc799","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**12. Inference**\n\nInference is the process of using a trained model to make predictions on new data. This cell demonstrates how to use the trained Transformer model to generate text.\n\nThe following code shows how to perform inference with the trained Transformer model:","metadata":{"_uuid":"48619d31-037b-45f9-b3b1-8c0fe615a112","_cell_guid":"bbbb9253-cc5a-445a-9b42-238e26da8da8","id":"neSqS16X1Utd","trusted":true}},{"cell_type":"code","source":"vocab_size = 50257\nembed_dim = 768\nmax_len = 1024\nembed_dropout = 0.1\nnum_blocks = 12  # or 24 for GPT-2 XL\nnum_heads = 12   # or 24 for GPT-2 XL\nff_dim = 3072\nattn_dropout = 0.1\nff_dropout = 0.1\n\n# Initialize GPT model\nmodel = GPT(\n    vocab_size=vocab_size,\n    embed_dim=embed_dim,\n    max_len=max_len,\n    embed_dropout=embed_dropout,\n    num_blocks=num_blocks,\n    num_heads=num_heads,\n    ff_dim=ff_dim,\n    attn_dropout=attn_dropout,\n    ff_dropout=ff_dropout\n)","metadata":{"_uuid":"731a0248-e7cc-4065-be4a-d87216fb662d","_cell_guid":"626c53c4-8e3a-482c-968d-fdead473a09e","collapsed":false,"id":"KIR6Xiftpsux","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code performs several key steps in preparing and setting up a PyTorch DataLoader for training a language model using the GPT-2 architecture. Heres a detailed explanation of each step:\n\n1. **Importing Necessary Libraries**:\n   - `torch`: The core library of PyTorch, used for tensor operations and building neural networks.\n   - `DataLoader` and `TensorDataset` from `torch.utils.data`: Tools to handle and process data efficiently.\n\n2. **Initializing Tokenizer and Model**:\n   - `model_name = \"gpt2\"`: Specifies the name of the pre-trained model to be used.\n   - `tokenizer = AutoTokenizer.from_pretrained(model_name)`: Loads the pre-trained tokenizer for GPT-2, which is responsible for converting text into token IDs that the model can understand.\n   - `model = AutoModelForCausalLM.from_pretrained(model_name)`: Loads the pre-trained GPT-2 model, which is designed for causal language modeling tasks.\n\n3. **Setting Pad Token**:\n   - If the tokenizer does not have a pad token, it sets the pad token to be the same as the end-of-sequence (eos) token. This ensures that sequences are padded correctly to a uniform length.\n\n4. **Tokenizing the Data**:\n   - `inputs = tokenizer(sample_data, return_tensors='pt', max_length=128, padding='max_length', truncation=True)`: Tokenizes the sample data, converts it to PyTorch tensors, and ensures all sequences are padded or truncated to a maximum length of 128 tokens.\n\n5. **Creating TensorDataset**:\n   - `input_ids = inputs['input_ids']`: Extracts the input IDs (tokenized data) from the tokenized inputs.\n   - `labels = inputs['input_ids']`: Sets the labels to be the same as the input IDs for language modeling tasks, where the model tries to predict the next token in the sequence.\n   - `train_dataset = TensorDataset(input_ids, labels)`: Creates a TensorDataset from the input IDs and labels, which will be used to create a DataLoader.\n\n6. **Creating DataLoader**:\n   - `train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)`: Creates a DataLoader from the TensorDataset, specifying a batch size of 2 and shuffling the data at the beginning of each epoch to ensure the model sees the data in a different order each time.\n\nThis setup is crucial for efficiently feeding data into the model during training, ensuring that the data is properly tokenized, padded, and batched.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Tokenizer and model\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize the data\ninputs = tokenizer(sample_data, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n\n# Create TensorDataset\ninput_ids = inputs['input_ids']\nlabels = inputs['input_ids']  # In this case, labels are the same as input_ids for language modeling\n\n# Create DataLoader\ntrain_dataset = TensorDataset(input_ids, labels)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)","metadata":{"_uuid":"6110391c-a629-4240-80f5-01d253ab27b2","_cell_guid":"cb12c825-6d3c-4e86-977c-54a05c36db25","collapsed":false,"id":"LRlzrDFtp5D8","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. **Sample Input Text**:\n   - `input_txt = \"Building Deep Neural Networks with PyTorch\"`: Defines the sample text that you want to tokenize and process. This text will be converted into token IDs that the model can understand.\n\n2. **Tokenizing the Input Text**:\n   - `input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)`: Uses the tokenizer to convert the input text into token IDs. The `return_tensors=\"pt\"` argument ensures that the token IDs are returned as a PyTorch tensor. The `to(device)` method moves the tensor to the specified device (CPU or GPU) for further processing or inference.\n\n3. **Printing the Token IDs and Shape**:\n   - `print(input_ids)`: Prints the token IDs generated from the input text. These IDs represent the input text in a numerical format that the model can process.\n   - `print(input_ids.shape)`: Prints the shape of the `input_ids` tensor. This helps in understanding the dimensions of the tensor, which is useful for verifying that the tokenization process worked correctly.\n\nThis is essential for preparing textual data for input into a neural network model, particularly in natural language processing tasks. By converting the text into token IDs and ensuring the data is in the correct format, the model can effectively process the input and generate meaningful predictions or outputs.","metadata":{}},{"cell_type":"code","source":"input_txt = \"Building Deep Neural Networks with PyTorch\"\n\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\nprint(input_ids)\nprint(input_ids.shape)","metadata":{"_uuid":"9fe33d08-cfbc-4009-94ec-2b7ef977fc36","_cell_guid":"ce5bbc32-037a-42cb-b82b-624cb2484736","collapsed":false,"id":"NoAJi4i1p8_x","outputId":"c57d254a-edf1-4c4d-a153-a6c722511365","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now performing text generation using a pre-trained language model. It generates new text by predicting the next token iteratively and appending it to the input sequence. Heres a detailed explanation of each part:\n\n1. **Moving Model to Device**:\n   - `model = model.to(device)`: Moves the model to the specified device (CPU or GPU). This is important for efficient computation, especially when using a GPU.\n\n2. **Setting Up Iteration Parameters**:\n   - `iterations = []`: Initializes an empty list to store the results of each iteration.\n   - `n_steps = 10`: Sets the number of steps (tokens) to generate. This determines how many new tokens will be generated.\n   - `choices_per_step = 5`: Sets the number of top token choices to store for each step. This is useful for analyzing the models predictions.\n\n3. **Text Generation Loop**:\n   - `with torch.no_grad()`: Disables gradient computation, which is not needed during inference, to save memory and computation.\n   - `for _ in range(n_steps)`: Loops for the specified number of steps to generate new tokens iteratively.\n\n4. **Decoding and Forward Pass**:\n   - `iteration = dict()`: Initializes a dictionary to store the results of the current iteration.\n   - `iteration[\"Input\"] = tokenizer.decode(input_ids[0])`: Decodes the current input IDs to the corresponding text and stores it.\n   - `output = model(input_ids=input_ids)`: Performs a forward pass through the model with the current input IDs to get the output logits.\n\n5. **Processing Output Logits**:\n   - `logits = output.logits`: Extracts the logits from the model output. Logits are the raw, unnormalized predictions of the model.\n   - `next_token_logits = logits[0, -1, :]`: Selects the logits corresponding to the last token in the sequence.\n   - `next_token_probs = torch.softmax(next_token_logits, dim=-1)`: Applies the softmax function to the logits to convert them into probabilities.\n   - `sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)`: Sorts the token probabilities in descending order to get the most likely next tokens.\n\n6. **Storing Top Token Choices**:\n   - Loops over the top token choices to store their probabilities and decoded text representations in the `iteration` dictionary.\n   - `iterations.append(iteration)`: Appends the current iterations results to the `iterations` list.\n\n7. **Appending Predicted Token to Input**:\n   - `input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)`: Appends the most likely next token to the input sequence for the next iteration.\n\n8. **Creating DataFrame for Analysis**:\n   - `sample_inference = pd.DataFrame(iterations)`: Converts the `iterations` list into a pandas DataFrame for easier analysis and visualization.\n   - `sample_inference.head()`: Displays the first few rows of the DataFrame to inspect the results.\n\nThis simply shows how to generate text using a pre-trained language model by iteratively predicting and appending new tokens to the input sequence. The use of softmax ensures that the models predictions are probabilistic, allowing for more controlled and interpretable text generation.","metadata":{}},{"cell_type":"code","source":"model = model.to(device)\niterations = []\nn_steps = 10\nchoices_per_step = 5\n\nwith torch.no_grad():\n    for _ in range(n_steps):\n        iteration = dict()\n        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n        output = model(input_ids=input_ids)\n\n        # Extract the logits from the output\n        logits = output.logits\n\n        # Select logits of the first batch and the last token and apply softmax to get the probability\n        next_token_logits = logits[0, -1, :]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n\n        # Store tokens with highest probabilities in our little table\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy()\n            token_choice = (\n                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n            )\n            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n        iterations.append(iteration)\n\n        # Append predicted next token to input\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n\nsample_inference = pd.DataFrame(iterations)\nsample_inference.head()\n","metadata":{"_uuid":"15e96dbb-e7d6-4b78-84b6-84da2b164387","_cell_guid":"0e870499-a8c5-4d30-84f1-dd2c0e99c668","collapsed":false,"id":"zemsMWd4qDX3","outputId":"85ec5ba4-a0fb-4364-a0da-6900b2a7192a","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function Parameters:\n- `input_text` (str): The initial input text to start the generation process.\n- `model`: The pre-trained language model used for text generation.\n- `tokenizer`: The tokenizer corresponding to the model, used to convert text to token IDs and vice versa.\n- `max_length` (int): The maximum length of the generated text. The function will stop generating new tokens once this length is reached.\n- `device` (str): The device on which the computation will be performed (e.g., 'cpu' or 'cuda').\n\n### Function Workflow:\n\n1. **Move Model to Device**:\n   - `model = model.to(device)`: Moves the model to the specified device (CPU or GPU) for efficient computation.\n\n2. **Tokenize Input Text**:\n   - `input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)`: Tokenizes the input text, converts it to a PyTorch tensor, and moves it to the specified device. The `return_tensors='pt'` argument ensures that the token IDs are returned as a PyTorch tensor.\n\n3. **Initialize Variables**:\n   - `end_token_id = tokenizer.eos_token_id`: Retrieves the end-of-sequence token ID from the tokenizer. This token indicates the end of the generated sequence.\n   - `generated_ids = input_ids.flatten().clone()`: Flattens the input IDs to a 1-dimensional tensor and clones it to initialize the generated sequence.\n\n4. **Disable Gradient Computation**:\n   - `with torch.no_grad()`: Disables gradient computation to save memory and computation during inference, as gradients are not needed.\n\n5. **Text Generation Loop**:\n   - The loop continues until the end-of-sequence token is generated or the generated sequence reaches the specified maximum length.\n\n6. **Forward Pass**:\n   - `output = model(input_ids=input_ids)`: Performs a forward pass through the model with the current input IDs to get the output logits.\n\n7. **Extract and Process Logits**:\n   - `logits = output.logits`: Extracts the logits from the model output. Logits are the raw, unnormalized predictions of the model.\n   - `next_token_logits = logits[:, -1, :]`: Selects the logits corresponding to the last token in the sequence.\n\n8. **Predict Next Token**:\n   - `next_token_id = torch.argmax(next_token_logits, dim=-1)`: Selects the token ID with the highest probability (argmax) from the logits of the last token.\n   - `generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)`: Appends the predicted token ID to the generated sequence.\n   - `input_ids = next_token_id.unsqueeze(0)`: Prepares the predicted token ID as the input for the next iteration.\n\n9. **Check Stopping Condition**:\n   - The loop breaks if the end-of-sequence token is generated or the generated sequence reaches the specified maximum length.\n\n10. **Decode Generated Text**:\n    - `generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)`: Decodes the generated token IDs back to text, skipping special tokens like the end-of-sequence token.\n\n11. **Return Generated Text**:\n    - `return generated_text`: Returns the generated text.\n\nNow we know how to use a pre-trained language model to generate text by iteratively predicting the next token and appending it to the input sequence. The use of softmax ensures that the models predictions are probabilistic, allowing for controlled and interpretable text generation.","metadata":{}},{"cell_type":"code","source":"def generate_text_until_end(\n    input_text: str,\n    model,\n    tokenizer,\n    max_length: int = 100,\n    device='cpu',\n):\n    model = model.to(device)\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n    end_token_id = tokenizer.eos_token_id\n    generated_ids = input_ids.flatten().clone()  # Convert to 1-dimensional tensor\n\n    with torch.no_grad():\n        while True:\n            output = model(input_ids=input_ids)\n            logits = output.logits  # Extract the logits\n\n            next_token_logits = logits[:, -1, :]  # Select logits of the last token\n            next_token_id = torch.argmax(next_token_logits, dim=-1)\n            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n            input_ids = next_token_id.unsqueeze(0)\n\n            if next_token_id == end_token_id or len(generated_ids) >= max_length:\n                break\n\n    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    return generated_text","metadata":{"_uuid":"2298fe43-9eeb-443c-b75b-17b66318c451","_cell_guid":"aeecc4ac-5c76-4ac6-85ed-2adbdc829589","collapsed":false,"id":"I2fi9cSYqLw7","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Text:\n\n1. **Function Call**:\n   - `generated_text = generate_text_until_end(...)`: Calls the `generate_text_until_end` function with the specified parameters to generate text.\n\n2. **Input Parameters**:\n   - `input_text=\"I like to eat\"`: Specifies the initial input text to start the generation process. This is the prompt that the model will continue from.\n   - `model=model`: The pre-trained language model to be used for text generation.\n   - `tokenizer=tokenizer`: The tokenizer corresponding to the model, used to convert text to token IDs and vice versa.\n   - `max_length=20`: The maximum length of the generated text. The function will stop generating new tokens once this length is reached.\n   - `device=device`: The device on which the computation will be performed (e.g., 'cpu' or 'cuda').\n\n3. **Printing Generated Text**:\n   - `print(generated_text)`: Prints the generated text to the console.\n\n### Why though?:\n\nThe purpose of this code snippet is to demonstrate how to use the `generate_text_until_end` function to generate a sequence of text starting from a given prompt. The function iteratively predicts and appends new tokens to the input sequence until the specified maximum length is reached or an end-of-sequence token is generated. The generated text is then printed to the console for inspection.\n\nBy specifying the `max_length`, the user controls the length of the generated sequence, ensuring that the output is of manageable size. The use of the pre-trained language model allows for coherent and contextually relevant text generation based on the given prompt.","metadata":{}},{"cell_type":"code","source":"generated_text = generate_text_until_end(\n    input_text=\"I like to eat\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=20,\n#     k=50,  # Top-k value\n    device=device,\n)\n\nprint(generated_text)","metadata":{"_uuid":"efb08d5a-06e2-45e7-9922-6b1b9ae15d72","_cell_guid":"596ca8a5-b0c6-4b79-9b18-3b738380398c","collapsed":false,"id":"l7KHlOYIqNjA","outputId":"5790d355-021d-421d-aed8-78dcb48b11ff","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Congratulations!! for how far you have come.** <br>\n**Feel The AGI!**\n\n**Author(s):**\n**Adiza Alhassan And Jason Quist**\n\n\n*This notebook was originally created by Ghana Data Science Summit for the* *IndabaX Ghana 2024 Conference and is published under MIT license. *","metadata":{"_uuid":"4b423384-c3e2-46dc-b00f-4aba192d1acb","_cell_guid":"9ac30dc9-41bd-4737-96f4-bffb98746b69","id":"BRiLZMiR2bao","trusted":true}}]}