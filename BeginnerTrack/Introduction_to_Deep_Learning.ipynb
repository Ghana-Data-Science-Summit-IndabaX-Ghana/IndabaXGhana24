{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Introduction\n",
        "\n",
        "\n",
        "## Introduction to Building and Training a Transformer Model with PyTorch\n",
        "\n",
        "\n",
        "Transformers, especially exemplified by models like GPT-2, have revolutionized natural language processing tasks such as text generation, translation, and summarization. This notebook provides a comprehensive tutorial on constructing and training a Transformer model from scratch using PyTorch. It covers essential components like input embeddings, positional encoding, multi-head attention, feedforward blocks, and more. By the end, you'll have a foundational understanding of how these components come together to form a powerful language model."
      ],
      "metadata": {
        "id": "PrNd_TGM6KRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning Training**"
      ],
      "metadata": {
        "id": "WgCnwbOAr9x2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "1. [Input Embeddings](#input-embeddings)\n",
        "2. [Positional Encoding](#positional-encoding)\n",
        "3. [Layer Normalization](#layer-normalization)\n",
        "4. [Feed Forward Block](#feed-forward-block)\n",
        "5. [Multi-Head Attention Block](#multi-head-attention-block)\n",
        "6. [Residual Connection](#residual-connection)\n",
        "7. [Projection Head](#projection-head)\n",
        "8. [Transformer Block](#transformer-block)\n",
        "9. [Building the Transformer](#building-the-transformer)\n",
        "10. [Sample Usage](#sample-usage)\n",
        "11. [Training the Transformer](#training-the-transformer)\n",
        "    1. [Data Preprocessing](#data-preprocessing)\n",
        "    2. [Model Training](#model-training)\n",
        "12. [Inference](#inference)"
      ],
      "metadata": {
        "id": "g3Zq5nnTxxzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "TbtDtHvB6as2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRy7FCqznlY1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Input** **Embeddings**\n",
        "\n",
        "This cell initializes the embedding layer that converts tokenized input into dense numerical vectors. Embeddings capture semantic relationships between tokens and are crucial for the Transformer model to understand the meaning of words in the input sequence."
      ],
      "metadata": {
        "id": "SppqCbVkwUvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim: int, vocab_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the InputEmbedding module.\n",
        "\n",
        "        Args:\n",
        "            embed_dim (int): The dimensionality of the input embedding.\n",
        "            vocab_size (int): The size of the vocabulary.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Store the dimensionality and vocabulary size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Create an embedding layer that maps the vocabulary to a embed_dim-dimensional space\n",
        "        # The embedding layer should have shape (vocab_size, embed_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the InputEmbedding module.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tensor: The embedded input tensor after scaling it by the square root of the dimensionality.\n",
        "\n",
        "        \"\"\"\n",
        "        # Embed the input tensor using the embedding layer\n",
        "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
        "        embedded_input = self.embedding(x)\n",
        "        # Scale the embedded input tensor by the square root of the dimensionality\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        scaled_embedded_input = embedded_input * torch.sqrt(torch.tensor(self.embed_dim))\n",
        "        return scaled_embedded_input"
      ],
      "metadata": {
        "id": "CO1r2FtaoRCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Positional Encoding**\n",
        "\n",
        "This cell computes positional encodings that add information about the order of tokens in the sequence. Positional encodings are necessary because Transformers don't inherently understand the sequential nature of data like RNNs; these encodings help maintain sequential information.\n",
        "\n"
      ],
      "metadata": {
        "id": "cvlS2mFoyJxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 512, max_seq_len: int = 100, dropout: float = 0.1,):\n",
        "        \"\"\"Initialize the PositionalEncoding module.\"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Precompute the positional encoding matrix\n",
        "        self.positional_encoding = self._precompute_positional_encoding(max_seq_len, embed_dim)\n",
        "\n",
        "    def _precompute_positional_encoding(self, max_seq_len, embed_dim):\n",
        "        \"\"\"Precompute the positional encoding matrix.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Create a positional encoding matrix of shape (max_seq_len, embed_dim)\n",
        "            positional_encoding = torch.zeros(max_seq_len, embed_dim)\n",
        "            # Create a tensor 'pos' with values [0, 1, 2, ..., max_seq_len - 1] (max_seq_len, 1)\n",
        "            position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "            # Compute the positional encoding matrix\n",
        "            division_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "            positional_encoding[:, 0::2] = torch.sin(position * division_term)\n",
        "            positional_encoding[:, 1::2] = torch.cos(position * division_term)\n",
        "            # Shape (max_seq_len, embed_dim) -> (1, max_seq_len, embed_dim)\n",
        "            positional_encoding = positional_encoding.unsqueeze(0)\n",
        "\n",
        "        return positional_encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass of the PositionalEncoding module.\"\"\"\n",
        "        # Add the positional encoding matrix to the input tensor\n",
        "        x = x + self.positional_encoding[:, : x.size(1)].to(x.device)\n",
        "        # Apply dropout to the input tensor\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aJrkmLCboUdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Layer Normalization**\n",
        "\n",
        "Implements layer normalization, which normalizes activations across the features. This improves training stability by reducing the internal covariate shift problem, allowing each layer of the model to learn more independently of others."
      ],
      "metadata": {
        "id": "EoVCS1LiyZ57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, embed_dim: int, eps: float = 1e-6):\n",
        "        \"\"\"Initialize the LayerNormalization module.\"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # Create two learnable parameters to scale and shift the normalized input\n",
        "        self.gain = nn.Parameter(torch.Tensor(embed_dim).uniform_())  # Initialize with values sampled from a uniform distribution\n",
        "        self.bias = nn.Parameter(torch.Tensor(embed_dim).normal_())    # Initialize with values sampled from a normal distribution\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass of the LayerNormalization module.\"\"\"\n",
        "        # Compute the mean and standard deviation of the input tensor\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # Zero center by subtracting the mean from the input tensor\n",
        "        # Normalize scale by dividing by the standard deviation and add epsilon for numerical stability\n",
        "        # Scale and shift the normalized input using the learnable parameters\n",
        "        return (x - mean) / (std + self.eps) * self.gain + self.bias"
      ],
      "metadata": {
        "id": "xROh0AlIoXxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Feed Forward Block**\n",
        "\n",
        "Defines a feedforward neural network block, which consists of two linear transformations with a ReLU activation function in between. This block facilitates nonlinear transformations of feature representations learned by the Transformer model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AqVxXz-_yfu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, embed_dim: int, intermediate_size: int, dropout: float = 0.1):\n",
        "        \"\"\"Initialize the FeedForwardBlock module.\n",
        "        embed_dim is the hidden size of the transformer model functions as input and output size of the FeedForwardBlock\n",
        "        intermediate_size is the hidden size of the intermediate layer in the FeedForwardBlock\n",
        "        dropout is the dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # embed_dim is the dimensionality of the input and output of the FeedForwardBlock\n",
        "        # intermediate_size is the dimensionality of the intermediate layer in the FeedForwardBlock\n",
        "        self.fc1 = nn.Linear(embed_dim, intermediate_size) # W1 and B1 in the formula\n",
        "        self.fc2 = nn.Linear(intermediate_size, embed_dim) # W2 and B2 in the formula\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass of the FeedForwardBlock module.\"\"\"\n",
        "        # (Batch, Seq_len, embed_dim) -> (Batch, Seq_len, intermediate_size) -> (Batch, Seq_len, embed_dim)\n",
        "        x_intermediate = self.dropout(F.relu(self.fc1(x)))\n",
        "        x_output = self.fc2(x_intermediate)\n",
        "        return x_output"
      ],
      "metadata": {
        "id": "w5AAMcrloZPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Multi-Head Attention Block**\n",
        "\n",
        "Implements multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This mechanism enhances the model's ability to capture dependencies and relationships in the input data."
      ],
      "metadata": {
        "id": "8LODeG5Clvni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 512, num_heads: int = 8, attn_dropout: float = 0.1, ff_dropout: float = 0.1, max_len: int = 512):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % self.num_heads == 0, \"invalid heads and embedding dimension configuration\"\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.proj_dropout = nn.Dropout(ff_dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        # Apply linear transformations to the input tensor\n",
        "        # Take input tensor and apply linear transformation,\n",
        "        # then split the tensor into num_heads and head_dim\n",
        "        # transpose the tensor into correct order\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, num_heads, head_dim) ->\n",
        "        # (batch_size, seq_len, num_heads, head_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
        "        q = self.query(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        k = self.key(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "        v = self.value(x).view(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores using Einsum\n",
        "        # b: batch size, h: num_heads, i: seq_len, j: seq_len, d: head_dim\n",
        "        # Multiply query and key tensors element-wise and sum along the shared dimension (head_dim)\n",
        "        # Divide by the square root of the dimension of the query/key vectors\n",
        "        # Equivalent to: attention = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n",
        "        # Shape: (batch_size, num_heads, seq_len, head_dim) * (batch_size, num_heads, seq_len, head_dim)\n",
        "        # -> (batch_size, num_heads, seq_len, seq_len)\n",
        "        attention = torch.einsum('bhid,bhjd->bhij', q, k) / math.sqrt(q.size(-1))\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        # Apply softmax and dropout\n",
        "        # Shape: (batch_size, num_heads, seq_len, seq_len) -> (batch_size, num_heads, seq_len, head_dim)\n",
        "        attention = self.attn_dropout(F.softmax(attention, dim=-1))\n",
        "\n",
        "        # Compute the weighted sum of values using attention scores\n",
        "        # Equivalent to: torch.matmul(attention, v)\n",
        "        # Shape: (batch_size, num_heads, seq_len, seq_len) * (batch_size, num_heads, seq_len, head_dim)\n",
        "        # -> (batch_size, num_heads, seq_len, head_dim)\n",
        "        y = torch.einsum('bhij,bhjd->bhid', attention, v)\n",
        "\n",
        "        # Merge the num_heads and head_dim back to the embed_dim\n",
        "        # Transpose sequence length and num_heads\n",
        "        # Flatten out the full tensor\n",
        "        # Reshape based on batch size, sequence length and embed_dim\n",
        "        # Shape: (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, num_heads, head_dim)\n",
        "        # -> (batch_size, seq_len, num_heads * head_dim)\n",
        "        # -> (batch_size, seq_len, embed_dim)\n",
        "        y = y.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "\n",
        "        # Apply linear transformation and dropout\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        return self.proj_dropout(self.proj(y))"
      ],
      "metadata": {
        "id": "Y1QgdrDKouit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**6. Residual Connection**\n",
        "\n",
        "Defines a residual connection that adds the input to the output of a sub-layer before applying layer normalization. This helps in mitigating the vanishing gradient problem and allows gradients to flow more effectively during training."
      ],
      "metadata": {
        "id": "SG-O99lzzFGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout: float = 0.1):\n",
        "        \"\"\"Initialize the ResidualConnection module.\"\"\"\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNormalization(embed_dim=embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"Perform the forward pass of the ResidualConnection module.\"\"\"\n",
        "        # Apply sublayer (e.g., feedforward block)\n",
        "        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        sublayer_output = sublayer(x)\n",
        "        # Apply layer normalization\n",
        "        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        normalized_x = self.layer_norm(x)\n",
        "        # Add residual connection\n",
        "        # (batch_size, seq_len, embed_dim) + (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        residual_output = normalized_x + sublayer_output\n",
        "        # Apply dropout to the sum\n",
        "        return self.dropout(residual_output)"
      ],
      "metadata": {
        "id": "G8cg1ZanowOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Projection Head**\n",
        "\n",
        "Defines the projection head, which projects the model's hidden representations back into the original vocabulary space for output generation. This final layer ensures that the model's output is interpretable in terms of the input tokens."
      ],
      "metadata": {
        "id": "lDeMGfZ4zOpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, embed_dim: int, vocab_size: int):\n",
        "        \"\"\"Initialize the ProjectionHead module.\"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform the forward pass of the ProjectionHead module.\"\"\"\n",
        "        # Apply linear transformation to the input tensor\n",
        "        # (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, vocab_size)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "fj8rp6cLo2RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Transformer Block**\n",
        "\n",
        "Combines all the previously defined components (multi-head attention, feedforward block, residual connection, and layer normalization) into a single Transformer block. This block forms the core building block of the Transformer architecture, facilitating the model's ability to process and generate text."
      ],
      "metadata": {
        "id": "R1p78gyWw8jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        ff_dim: int = 2048,\n",
        "        attn_dropout: float = 0.1,\n",
        "        ff_dropout: float = 0.1,\n",
        "        dropout: float = 0.1,\n",
        "        max_len: int = 512,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Initialize multi-head self-attention mechanism\n",
        "        self.MultiHeadAttention = MultiHeadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            max_len=max_len,\n",
        "            )\n",
        "        # Initialize feed-forward block\n",
        "        self.feed_forward = FeedForwardBlock(\n",
        "            embed_dim=embed_dim,\n",
        "            intermediate_size=ff_dim,\n",
        "            dropout=ff_dropout,\n",
        "            )\n",
        "        # Initialize residual connections\n",
        "        self.residual_connection1 = ResidualConnection(embed_dim=embed_dim, dropout=dropout)\n",
        "        self.residual_connection2 = ResidualConnection(embed_dim=embed_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        # Apply self-attention mechanism with residual connection\n",
        "        x_with_attention = self.residual_connection1(x, lambda x: self.MultiHeadAttention(x, mask=attention_mask))\n",
        "        # Apply feed-forward block with residual connection\n",
        "        x_with_ff = self.residual_connection2(x_with_attention, self.feed_forward)\n",
        "        return x_with_ff"
      ],
      "metadata": {
        "id": "XahbnmqHo3k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Building the Transformer**\n",
        "\n",
        "Constructs the entire Transformer architecture by stacking multiple Transformer blocks. This setup creates a deep neural network capable of processing sequential data and generating output predictions based on learned patterns and relationships."
      ],
      "metadata": {
        "id": "pNgYNrR9zaTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 512,\n",
        "        max_len: int = 512,\n",
        "        embed_dropout: float = 0.1,\n",
        "        num_blocks: int = 6,\n",
        "        num_heads: int = 8,\n",
        "        ff_dim: int = 2048,\n",
        "        attn_dropout: float = 0.1,\n",
        "        ff_dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.token_embedding = InputEmbedding(\n",
        "            embed_dim=embed_dim,\n",
        "            vocab_size=vocab_size,\n",
        "            )\n",
        "        self.positional_embedding = PositionalEncoding(\n",
        "            embed_dim=embed_dim,\n",
        "            max_seq_len=max_len,\n",
        "            dropout=embed_dropout,\n",
        "            )\n",
        "        self.blocks = nn.ModuleList([DecoderBlock(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            attn_dropout=attn_dropout,\n",
        "            ff_dropout=ff_dropout,\n",
        "            max_len=max_len,\n",
        "            ) for _ in range(num_blocks)])\n",
        "\n",
        "        self.projection_head = ProjectionHead(embed_dim=embed_dim, vocab_size=vocab_size)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        # Shape: (batch_size, seq_len) -> (seq_len)\n",
        "        seq_len = input_ids.size(1)\n",
        "        assert seq_len <= self.max_len, \"Sequence longer than model capacity\"\n",
        "\n",
        "        # Token embedding\n",
        "        # Shape: (batch_size, seq_len) -> (batch_size, seq_len, embed_dim)\n",
        "        x = self.token_embedding(input_ids)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Add positional embedding\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        x = self.positional_embedding(x)\n",
        "\n",
        "        # Forward through decoder blocks\n",
        "        # output of each block is the hidden state of the transformer\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, attention_mask=attention_mask)\n",
        "\n",
        "        # Linear layer for output logits\n",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, vocab_size)\n",
        "        x = self.projection_head(x)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-Xd1UOAKpA2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Sample Usage**\n",
        "\n",
        " Demonstrates how to initialize and use the Transformer model for tasks such as text generation and prediction. This cell provides a practical example of how to interact with the Transformer architecture once it's been constructed."
      ],
      "metadata": {
        "id": "Mz61S0D1zgJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "vocab_size = 50257  # Example vocab size; specific to GPT2 tokenizer\n",
        "embed_dim = 768\n",
        "max_len = 1024 # This can be adjusted based on the use case\n",
        "embed_dropout = 0.1\n",
        "num_blocks = 6 # This can be adjusted based on the use case\n",
        "num_heads = 8 # This can be adjusted based on the use case\n",
        "ff_dim = 2048 # This can be adjusted based on the use case\n",
        "attn_dropout = 0.1\n",
        "ff_dropout = 0.1\n",
        "\n",
        "# Initialize GPT model\n",
        "model = GPT(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    max_len=max_len,\n",
        "    embed_dropout=embed_dropout,\n",
        "    num_blocks=num_blocks,\n",
        "    num_heads=num_heads,\n",
        "    ff_dim=ff_dim,\n",
        "    attn_dropout=attn_dropout,\n",
        "    ff_dropout=ff_dropout\n",
        ")"
      ],
      "metadata": {
        "id": "K3h2icNIpCSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Training the Transformer**\n",
        "\n",
        "**11.1 Data Preprocessing**"
      ],
      "metadata": {
        "id": "t63qvcsqzo43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "    \"Mary had a little lamb\",\n",
        "    \"Its fleece was white as snow\",\n",
        "    \"And everywhere that Mary went\",\n",
        "    \"The lamb was sure to go\",\n",
        "]"
      ],
      "metadata": {
        "id": "RXq_8wNfpI3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, data:list, tokenizer, max_length:int):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.end_token = tokenizer.eos_token_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        input_txt = self.tokenizer(text, truncation=True, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
        "        text_len = input_txt.size(0)\n",
        "        if text_len < self.max_length:\n",
        "            padding_len = self.max_length - text_len\n",
        "            padding = torch.tensor([self.end_token] * padding_len)\n",
        "            input_ids = torch.cat((input_txt, padding), dim=0)\n",
        "            label = torch.cat((input_txt[1:], torch.tensor([self.end_token]), padding), dim=0)\n",
        "        else:\n",
        "            input_ids = input_txt[:self.max_length]\n",
        "            label = torch.cat((input_txt[1:self.max_length], torch.tensor([self.end_token])), dim=0)\n",
        "        return input_ids, label"
      ],
      "metadata": {
        "id": "K0gnEDjPpLbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "train_dataset = GPTDataset(\n",
        "    data = sample_data,\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = 200,\n",
        "    )"
      ],
      "metadata": {
        "id": "dTv_X60kpRnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, label = train_dataset[2]\n",
        "input_ids = input_ids.unsqueeze(0)\n",
        "label = label.unsqueeze(0)\n",
        "\n",
        "print(\"Label:\", label)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "print(\"Label Shape:\", label.shape)\n",
        "print(\"Input IDs Shape:\", input_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWjzHszMpVBP",
        "outputId": "bf68650e-26cc-4473-d3dd-025647e2fb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: tensor([[ 8347,   326,  5335,  1816, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
            "Input IDs: tensor([[ 1870,  8347,   326,  5335,  1816, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n",
            "Label Shape: torch.Size([1, 200])\n",
            "Input IDs Shape: torch.Size([1, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.2 Model Training**"
      ],
      "metadata": {
        "id": "nJ8QYo541BPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lr = 5e-5\n",
        "batch_size = 2\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "IGbwsSTUpZcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        # Unpack input and label from the batch and send them to the device\n",
        "        input_ids, labels = batch\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "        # Generate the causal mask\n",
        "        # Shape: (batch_size, seq_len, seq_len)\n",
        "        mask = generate_square_subsequent_mask(input_ids.size(1), device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(input_ids=input_ids, attention_mask=mask)\n",
        "\n",
        "        # Flatten the logits and labels for computing the loss\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        labels_flat = labels.view(-1)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(logits_flat, labels_flat)\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHwFX_z9pcJu",
        "outputId": "11006cd9-be81-4685-aa7c-d02f4a3fc799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 10.108588218688965\n",
            "Epoch 2/5, Loss: 7.591704368591309\n",
            "Epoch 3/5, Loss: 4.97402548789978\n",
            "Epoch 4/5, Loss: 2.2881640195846558\n",
            "Epoch 5/5, Loss: 0.5665177553892136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Inference**"
      ],
      "metadata": {
        "id": "neSqS16X1Utd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "embed_dim = 768\n",
        "max_len = 1024\n",
        "embed_dropout = 0.1\n",
        "num_blocks = 12  # or 24 for GPT-2 XL\n",
        "num_heads = 12   # or 24 for GPT-2 XL\n",
        "ff_dim = 3072\n",
        "attn_dropout = 0.1\n",
        "ff_dropout = 0.1\n",
        "\n",
        "# Initialize GPT model\n",
        "model = GPT(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    max_len=max_len,\n",
        "    embed_dropout=embed_dropout,\n",
        "    num_blocks=num_blocks,\n",
        "    num_heads=num_heads,\n",
        "    ff_dim=ff_dim,\n",
        "    attn_dropout=attn_dropout,\n",
        "    ff_dropout=ff_dropout\n",
        ")"
      ],
      "metadata": {
        "id": "KIR6Xiftpsux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "LRlzrDFtp5D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_txt = \"Machine Learning with PyTorch can do amazing\"\n",
        "\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "print(input_ids)\n",
        "print(input_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoAJi4i1p8_x",
        "outputId": "c57d254a-edf1-4c4d-a153-a6c722511365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[37573, 18252,   351,  9485, 15884,   354,   460,   466,  4998]])\n",
            "torch.Size([1, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "iterations = []\n",
        "n_steps = 10\n",
        "choices_per_step = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_steps):\n",
        "        iteration = dict()\n",
        "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "        output = model(input_ids=input_ids)\n",
        "\n",
        "        # Select logits of the first batch and the last token and apply softmax to get the probability\n",
        "        next_token_logits = output[0, -1, :]\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "\n",
        "        # Store tokens with highest probabilities in our little table\n",
        "        for choice_idx in range(choices_per_step):\n",
        "            token_id = sorted_ids[choice_idx]\n",
        "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
        "            token_choice = (\n",
        "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
        "            )\n",
        "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
        "        iterations.append(iteration)\n",
        "\n",
        "\n",
        "        # Append predicted next token to input\n",
        "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
        "\n",
        "sample_inference = pd.DataFrame(iterations)\n",
        "sample_inference.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zemsMWd4qDX3",
        "outputId": "85ec5ba4-a0fb-4364-a0da-6900b2a7192a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Input             Choice 1  \\\n",
              "0       Machine Learning with PyTorch can do amazing         bike (0.03%)   \n",
              "1  Machine Learning with PyTorch can do amazing bike         vell (0.03%)   \n",
              "2  Machine Learning with PyTorch can do amazing b...         bike (0.03%)   \n",
              "3  Machine Learning with PyTorch can do amazing b...       novice (0.04%)   \n",
              "4  Machine Learning with PyTorch can do amazing b...   Activities (0.03%)   \n",
              "\n",
              "             Choice 2          Choice 3            Choice 4  \\\n",
              "0   amusement (0.03%)        pg (0.03%)     Writing (0.02%)   \n",
              "1       wered (0.02%)      bike (0.02%)       remem (0.02%)   \n",
              "2       wered (0.03%)    novice (0.02%)       remem (0.02%)   \n",
              "3          mi (0.03%)   grazing (0.03%)      agency (0.02%)   \n",
              "4      novice (0.03%)   Writing (0.03%)   amusement (0.03%)   \n",
              "\n",
              "             Choice 5  \n",
              "0          mi (0.02%)  \n",
              "1   parasites (0.02%)  \n",
              "2   amusement (0.02%)  \n",
              "3        Crit (0.02%)  \n",
              "4         625 (0.03%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7c96987-1f49-47df-bee4-33e0a33ecbe7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "      <th>Choice 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Machine Learning with PyTorch can do amazing</td>\n",
              "      <td>bike (0.03%)</td>\n",
              "      <td>amusement (0.03%)</td>\n",
              "      <td>pg (0.03%)</td>\n",
              "      <td>Writing (0.02%)</td>\n",
              "      <td>mi (0.02%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Machine Learning with PyTorch can do amazing bike</td>\n",
              "      <td>vell (0.03%)</td>\n",
              "      <td>wered (0.02%)</td>\n",
              "      <td>bike (0.02%)</td>\n",
              "      <td>remem (0.02%)</td>\n",
              "      <td>parasites (0.02%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Machine Learning with PyTorch can do amazing b...</td>\n",
              "      <td>bike (0.03%)</td>\n",
              "      <td>wered (0.03%)</td>\n",
              "      <td>novice (0.02%)</td>\n",
              "      <td>remem (0.02%)</td>\n",
              "      <td>amusement (0.02%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Machine Learning with PyTorch can do amazing b...</td>\n",
              "      <td>novice (0.04%)</td>\n",
              "      <td>mi (0.03%)</td>\n",
              "      <td>grazing (0.03%)</td>\n",
              "      <td>agency (0.02%)</td>\n",
              "      <td>Crit (0.02%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Machine Learning with PyTorch can do amazing b...</td>\n",
              "      <td>Activities (0.03%)</td>\n",
              "      <td>novice (0.03%)</td>\n",
              "      <td>Writing (0.03%)</td>\n",
              "      <td>amusement (0.03%)</td>\n",
              "      <td>625 (0.03%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7c96987-1f49-47df-bee4-33e0a33ecbe7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7c96987-1f49-47df-bee4-33e0a33ecbe7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7c96987-1f49-47df-bee4-33e0a33ecbe7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1bcea451-3945-4c74-ae9c-718ee178584f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1bcea451-3945-4c74-ae9c-718ee178584f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1bcea451-3945-4c74-ae9c-718ee178584f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sample_inference",
              "summary": "{\n  \"name\": \"sample_inference\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Machine Learning with PyTorch can do amazing bikevell bike novice Activities reactingmimi\",\n          \"Machine Learning with PyTorch can do amazing bike\",\n          \"Machine Learning with PyTorch can do amazing bikevell bike novice Activities\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"vell (0.03%)\",\n          \"mi (0.04%)\",\n          \" bike (0.03%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"wered (0.02%)\",\n          \"lived (0.03%)\",\n          \" amusement (0.03%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" bike (0.02%)\",\n          \" enrichment (0.03%)\",\n          \"pg (0.03%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" remem (0.02%)\",\n          \"\\u30ed (0.02%)\",\n          \"Writing (0.02%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" 424 (0.02%)\",\n          \" parasites (0.02%)\",\n          \" remem (0.02%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_until_end(\n",
        "        input_text:str,\n",
        "        model:GPT,\n",
        "        tokenizer:AutoTokenizer,\n",
        "        max_length:int=100,\n",
        "        device='cpu',\n",
        "        ):\n",
        "    model = model.to(device)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    end_token_id = tokenizer.eos_token_id\n",
        "    generated_ids = input_ids.flatten().clone()  # Convert to 1-dimensional tensor\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            output = model(input_ids=input_ids)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            # Apply softmax to get probabilities but probably not necessary\n",
        "            # because the max value will still be the max value after softmax\n",
        "            # next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "            input_ids = next_token_id.unsqueeze(0)\n",
        "\n",
        "            if next_token_id == end_token_id or len(generated_ids) >= max_length:\n",
        "                break\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "I2fi9cSYqLw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "generated_text = generate_text_until_end(\n",
        "    input_text=\"I like to eat\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=20,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7KHlOYIqNjA",
        "outputId": "5790d355-021d-421d-aed8-78dcb48b11ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to eatums novice amusement minusmi amusement amusement amusementWritingpg amusement bikemi novice enrichment 625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations!! for how far you have comes.**\n",
        "\n",
        "**Author(s):**\n",
        "**Adiza Alhassan And Jason Quist**\n",
        "\n",
        "\n",
        "*This notebook was originally created by Ghana Data Science Summit for the* *IndabaX Ghana 2024 Conference and is published under MIT license. *"
      ],
      "metadata": {
        "id": "BRiLZMiR2bao"
      }
    }
  ]
}